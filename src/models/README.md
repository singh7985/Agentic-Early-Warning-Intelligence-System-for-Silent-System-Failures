# Models Module ‚Äî ML Training Infrastructure

This module provides a complete machine learning infrastructure for RUL (Remaining Useful Life) prediction with 5 models, comprehensive evaluation, MLflow tracking, and automated model selection.

## üì¶ Module Contents

```
src/models/
‚îú‚îÄ‚îÄ __init__.py           # Package initialization
‚îú‚îÄ‚îÄ baseline_ml.py        # XGBoost, Random Forest, Gradient Boosting
‚îú‚îÄ‚îÄ deep_learning.py      # LSTM, TCN neural networks
‚îú‚îÄ‚îÄ evaluation.py         # Metrics and visualization
‚îú‚îÄ‚îÄ mlflow_utils.py       # Experiment tracking
‚îî‚îÄ‚îÄ model_selector.py     # Model comparison and selection
```

## üöÄ Quick Start

### Train XGBoost

```python
from src.models.baseline_ml import XGBoostRULPredictor

model = XGBoostRULPredictor(n_estimators=200)
model.fit(X_train, y_train, X_val, y_val)
metrics = model.evaluate(X_test, y_test)
print(f"RMSE: {metrics['rmse']:.2f}")
```

### Train LSTM

```python
from src.models.deep_learning import LSTMRULPredictor, DeepLearningTrainer

model = LSTMRULPredictor(input_size=50)
trainer = DeepLearningTrainer(model)
history = trainer.train(X_train, y_train, X_val, y_val)
metrics = trainer.evaluate(X_test, y_test)
```

### Compare Models

```python
from src.models.model_selector import ModelSelector

selector = ModelSelector()
selector.add_model_results("XGBoost", train_metrics, test_metrics, inf_time, train_time)
selector.add_model_results("LSTM", train_metrics, test_metrics, inf_time, train_time)
best_model, scores = selector.select_best_model()
```

## üìä Available Models

### Baseline Models

1. **XGBoost RUL Predictor**
   - Gradient boosting with 200 estimators
   - Early stopping, feature importance
   - Fast inference (~0.015 ms)

2. **Random Forest RUL Predictor**
   - 200 tree ensemble
   - Robust to outliers
   - Parallel training

3. **Gradient Boosting RUL Predictor**
   - Sklearn-based boosting
   - Sequential learning
   - Strong baseline

### Deep Learning Models

4. **LSTM RUL Predictor**
   - 2-layer LSTM with 64 hidden units
   - Bidirectional option
   - GPU accelerated

5. **TCN RUL Predictor**
   - Temporal Convolutional Network
   - Dilated convolutions
   - Parallel training

## üéØ Key Features

- **Consistent API:** All models use `fit()`, `predict()`, `evaluate()`
- **9 Evaluation Metrics:** RMSE, MAE, R¬≤, MAPE, max error, median error, std error, early/late prediction %
- **6-Panel Visualizations:** Comprehensive prediction analysis
- **MLflow Integration:** Automatic experiment tracking
- **Model Selection:** Weighted scoring (70% performance + 30% stability)
- **Production Ready:** Save/load, error handling, logging

## üìö Documentation

- **[Technical Guide](../../docs/PHASE4_ML_TRAINING_GUIDE.md):** Complete documentation
- **[Quick Reference](../../docs/PHASE4_QUICK_REFERENCE.md):** Command cheatsheet
- **[Summary](../../docs/PHASE4_SUMMARY.md):** Executive overview
- **[Notebook](../../notebooks/03_ml_model_training.ipynb):** End-to-end demo

## üîß Installation

```bash
# Install dependencies
pip install xgboost torch mlflow scikit-learn matplotlib seaborn

# Or use requirements
pip install -r requirements.txt
```

## üìñ Usage Examples

### Complete Workflow

```python
# 1. Load data
from src.ingestion.cmapss_loader import CMAPSSDataLoader, prepare_cmapss_data
loader = CMAPSSDataLoader()
df_train, df_test = loader.load_dataset('FD001')
df_train_rul, df_test_rul, y_test = prepare_cmapss_data(df_train, df_test)

# 2. Feature engineering
from src.features.pipeline import FeatureEngineeringPipeline
pipeline = FeatureEngineeringPipeline()
X_train = pipeline.fit_transform(df_train_rul)
X_test = pipeline.transform(df_test_rul)

# 3. Train model
from src.models.baseline_ml import XGBoostRULPredictor
model = XGBoostRULPredictor()
model.fit(X_train, df_train_rul['RUL'])

# 4. Evaluate
metrics = model.evaluate(X_test, y_test)

# 5. Visualize
from src.models.evaluation import RULEvaluator
evaluator = RULEvaluator()
fig = evaluator.plot_predictions(y_test, model.predict(X_test))

# 6. Track with MLflow
from src.models.mlflow_utils import MLflowTracker
tracker = MLflowTracker()
with tracker.start_run():
    tracker.log_params(model.get_params())
    tracker.log_metrics(metrics)
```

### Model Comparison

```python
from src.models.model_selector import ModelSelector

selector = ModelSelector(primary_metric='rmse')

# Train and add all models
for model_name, model_class in models.items():
    model = model_class()
    model.fit(X_train, y_train, X_val, y_val)
    
    train_metrics = model.evaluate(X_train, y_train)
    test_metrics = model.evaluate(X_test, y_test)
    
    selector.add_model_results(
        model_name, train_metrics, test_metrics,
        inference_time, training_time
    )

# Select best
best_model, scores = selector.select_best_model()
selector.plot_comparison(save_path='outputs/comparison.png')
```

## üéì Module Details

### `baseline_ml.py`

**Classes:**
- `XGBoostRULPredictor`: XGBoost regressor wrapper
- `RandomForestRULPredictor`: Random Forest wrapper
- `GradientBoostingRULPredictor`: Gradient Boosting wrapper

**Methods:**
- `fit(X_train, y_train, X_val, y_val, early_stopping_rounds)`: Train model
- `predict(X)`: Make predictions
- `evaluate(X, y)`: Calculate metrics
- `get_feature_importance()`: Get feature rankings
- `save_model(path)`: Serialize model
- `load_model(path)`: Deserialize model

### `deep_learning.py`

**Classes:**
- `LSTMRULPredictor(nn.Module)`: LSTM neural network
- `TCNRULPredictor(nn.Module)`: TCN neural network
- `DeepLearningTrainer`: Training manager for PyTorch models
- `RULDataset(Dataset)`: PyTorch dataset wrapper

**Methods:**
- `train(X_train, y_train, X_val, y_val, epochs, batch_size)`: Train model
- `predict(X)`: Make predictions
- `evaluate(X, y)`: Calculate metrics
- `save_model(path)`: Save weights
- `load_model(path)`: Load weights

### `evaluation.py`

**Classes:**
- `RULEvaluator`: Comprehensive evaluation manager

**Methods:**
- `calculate_metrics(y_true, y_pred, prefix)`: Compute 9 metrics
- `plot_predictions(y_true, y_pred, title, save_path)`: 6-panel plot
- `plot_training_history(train_losses, val_losses)`: Loss curves
- `create_comparison_table(models_results)`: Multi-model DataFrame
- `plot_model_comparison(df, metrics)`: Bar chart comparison

### `mlflow_utils.py`

**Classes:**
- `MLflowTracker`: MLflow wrapper

**Methods:**
- `start_run(run_name, tags)`: Context manager for runs
- `log_params(params)`: Log hyperparameters
- `log_metrics(metrics, step)`: Log metrics
- `log_sklearn_model(model, path)`: Log sklearn model
- `log_pytorch_model(model, path)`: Log PyTorch model
- `log_figure(fig, file)`: Log matplotlib figure
- `log_artifact(path)`: Log file

**Functions:**
- `log_xgboost_model(tracker, model, params, metrics, features, name)`: Complete XGBoost logging
- `log_deep_learning_model(tracker, model, params, metrics, history, name)`: Complete DL logging

### `model_selector.py`

**Classes:**
- `ModelSelector`: Model comparison and selection

**Methods:**
- `add_model_results(name, train_metrics, test_metrics, inf_time, train_time, size)`: Register model
- `get_comparison_table()`: Get comparison DataFrame
- `select_best_model(stability_weight, performance_weight, max_inf_time)`: Select best model
- `plot_comparison(metrics, save_path)`: 6-panel dashboard
- `export_results(filepath)`: Save to CSV

## üîç Metrics Reference

| Metric | Description | Range | Interpretation |
|--------|-------------|-------|----------------|
| **RMSE** | Root Mean Squared Error | [0, ‚àû) | Lower is better |
| **MAE** | Mean Absolute Error | [0, ‚àû) | Lower is better |
| **R¬≤** | Coefficient of Determination | (-‚àû, 1] | Higher is better, 1 = perfect |
| **MAPE** | Mean Absolute % Error | [0, ‚àû) | Lower is better |
| **Max Error** | Maximum absolute error | [0, ‚àû) | Lower is better |
| **Median Error** | Median absolute error | [0, ‚àû) | Robust metric |
| **Std Error** | Std dev of errors | [0, ‚àû) | Lower = more consistent |
| **Early %** | % underestimates | [0, 100] | Balance with late % |
| **Late %** | % overestimates | [0, 100] | Balance with early % |

## üêõ Troubleshooting

### GPU Out of Memory
```python
# Reduce batch size
trainer.train(X, y, batch_size=64)

# Reduce model size
model = LSTMRULPredictor(hidden_size=32, num_layers=1)
```

### XGBoost Overfitting
```python
model = XGBoostRULPredictor(
    learning_rate=0.05,
    max_depth=4,
    reg_alpha=1.0
)
```

### LSTM Not Converging
```python
# Normalize data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)

# Increase learning rate
trainer = DeepLearningTrainer(model, learning_rate=0.01)
```

## üß™ Testing

```bash
# Run tests (when available)
pytest tests/test_models.py

# Check imports
python -c "from src.models import *; print('‚úì All imports successful')"
```

## üìù API Example

```python
# All models follow this interface
class ModelAPI:
    def fit(self, X_train, y_train, X_val=None, y_val=None):
        """Train the model"""
        pass
    
    def predict(self, X):
        """Make predictions"""
        return y_pred
    
    def evaluate(self, X, y):
        """Calculate metrics"""
        return {'rmse': float, 'mae': float, 'r2': float, 'nasa_score': float}
    
    def save_model(self, path):
        """Save model to disk"""
        pass
    
    def load_model(self, path):
        """Load model from disk"""
        pass
```

## üéØ Best Practices

1. **Always use group-aware train/val/test split**
   ```python
   from sklearn.model_selection import GroupShuffleSplit
   gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
   train_idx, val_idx = next(gss.split(X, y, groups=groups))
   X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
   y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
   ```

2. **Enable early stopping**
   ```python
   model.fit(X_train, y_train, X_val, y_val, early_stopping_rounds=20)
   ```

3. **Log all experiments**
   ```python
   with tracker.start_run():
       # Training code
       tracker.log_metrics(metrics)
   ```

4. **Compare multiple models**
   ```python
   selector = ModelSelector()
   # Add all models
   best_model = selector.select_best_model()
   ```

5. **Visualize predictions**
   ```python
   evaluator.plot_predictions(y_test, y_pred, save_path='outputs/plot.png')
   ```

## üìä Performance Benchmarks

**C-MAPSS FD001 Dataset (100 engines, ~20k cycles)**

| Model | RMSE | MAE | R¬≤ | Train Time | Inference (ms) |
|-------|------|-----|-----|------------|----------------|
| XGBoost | 25-30 | 18-22 | 0.83-0.87 | ~45s | ~0.015 |
| Random Forest | 28-33 | 20-25 | 0.80-0.85 | ~60s | ~0.05 |
| Gradient Boosting | 26-31 | 19-23 | 0.82-0.86 | ~50s | ~0.02 |
| LSTM | 27-32 | 19-24 | 0.81-0.86 | ~180s | ~0.023 |
| TCN | 26-31 | 18-23 | 0.82-0.87 | ~165s | ~0.021 |

## üîó Related Modules

- **[Ingestion](../ingestion/)**: Load C-MAPSS data
- **[Features](../features/)**: Feature engineering pipeline
- **[Config](../config.py)**: Configuration settings
- **[Logging](../logging_config.py)**: Logging setup

## üìö Further Reading

- [PHASE 4 Technical Guide](../../docs/PHASE4_ML_TRAINING_GUIDE.md)
- [Model Selection Algorithm](../../docs/PHASE4_ML_TRAINING_GUIDE.md#model-selection)
- [Hyperparameter Tuning](../../docs/PHASE4_ML_TRAINING_GUIDE.md#hyperparameter-tuning)
- [MLflow Integration](../../docs/PHASE4_ML_TRAINING_GUIDE.md#mlflow-integration)

## ü§ù Contributing

When adding new models:
1. Implement the standard API (`fit`, `predict`, `evaluate`, `save_model`, `load_model`)
2. Add docstrings and type hints
3. Include in model selector
4. Add MLflow logging support
5. Update documentation

## üìÑ License

Part of the Agentic Early Warning Intelligence System for Silent System Failures.

---

**Module Version:** 1.0  
**Last Updated:** 2024  
**Maintainer:** PHASE 4 Team
