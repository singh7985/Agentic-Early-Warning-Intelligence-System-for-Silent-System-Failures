{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26dfed69",
   "metadata": {},
   "source": [
    "# PHASE 2: Data Ingestion & Understanding\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers:\n",
    "1. **Time-Series Data**: NASA C-MAPSS turbofan engine degradation dataset\n",
    "2. **Text Data**: LogHub system logs (HDFS, BGL)\n",
    "\n",
    "We will:\n",
    "- Download and parse the C-MAPSS dataset\n",
    "- Visualize sensor degradation patterns\n",
    "- Create RUL (Remaining Useful Life) labels\n",
    "- Split data by engine (train/val/test)\n",
    "- Download and normalize LogHub datasets\n",
    "- Create incident narratives from logs\n",
    "- Generate synthetic maintenance reports\n",
    "\n",
    "**Timeline**: Days 4‚Äì7 of project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc0dba",
   "metadata": {},
   "source": [
    "# Section 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72ee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for datasets...\n",
      "‚úÖ Found C-MAPSS data in: /Users/xe/Desktop/CAPSTONE KRISHNA SIR /CMaps\n",
      "‚ö†Ô∏è No specific 'maintenance/log' text files found. (Will check data/raw/sample_logs.txt)\n",
      "\n",
      "üìã Schema Defined: 26 columns\n",
      "   Metadata: ['engine_id', 'cycle']\n",
      "   Sensors:  ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21'] (Count: 21)\n",
      "üìÇ Outputs will be saved to: reports/eda and data/interim\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure Visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# ==================== 1. AUTOMATIC DATASET DETECTION ====================\n",
    "print(\"üîç Scanning for datasets...\")\n",
    "\n",
    "# Define potential search paths (Project structure + User's Local Path)\n",
    "SEARCH_PATHS = [\n",
    "    Path(\"data/raw/CMAPSS\"),                            # Standard project path\n",
    "    Path(\"/Users/xe/Desktop/CAPSTONE KRISHNA SIR /CMaps/\"), # User's local path\n",
    "    Path(\"data/raw\"),                                   # Fallback\n",
    "    Path(\".\")                                           # Current dir\n",
    "]\n",
    "\n",
    "DATA_DIR = None\n",
    "for p in SEARCH_PATHS:\n",
    "    if p.exists() and list(p.glob(f\"train_FD001.txt\")):\n",
    "        DATA_DIR = p\n",
    "        print(f\"‚úÖ Found C-MAPSS data in: {DATA_DIR}\")\n",
    "        break\n",
    "\n",
    "if DATA_DIR is None:\n",
    "    raise FileNotFoundError(\"‚ùå Could not locate C-MAPSS files (train_FD001.txt, etc). Please check uploads.\")\n",
    "\n",
    "# Derive absolute project root from DATA_DIR: .../data/raw/CMAPSS ‚Üí project root\n",
    "PROJECT_ROOT     = DATA_DIR.resolve().parent.parent.parent\n",
    "INTERIM_DATA_DIR = PROJECT_ROOT / 'data' / 'interim'\n",
    "ED_REPORT_DIR    = PROJECT_ROOT / 'reports' / 'figures' / 'eda'\n",
    "INTERIM_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ED_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úÖ Project Root resolved to: {PROJECT_ROOT}\")\n",
    "\n",
    "# Scan for Text/Log Data\n",
    "TEXT_DATA_FILES = []\n",
    "for p in SEARCH_PATHS:\n",
    "    if p.exists():\n",
    "        logs = list(p.glob(\"*log*.txt\")) + list(p.glob(\"*maintenance*.txt\")) + list(p.glob(\"*doc*.txt\"))\n",
    "        if logs:\n",
    "            TEXT_DATA_FILES.extend(logs)\n",
    "\n",
    "if TEXT_DATA_FILES:\n",
    "    print(f\"‚úÖ Found extra text datasets: {[f.name for f in TEXT_DATA_FILES]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No specific 'maintenance/log' text files found. (Will check data/raw/sample_logs.txt)\")\n",
    "\n",
    "# ==================== 2. CONFIGURATION ====================\n",
    "# Columns based on C-MAPSS documentation (26 columns)\n",
    "# 1-2: Metadata (Engine, Cycle)\n",
    "# 3-5: Operational Settings\n",
    "# 6-26: Sensor Readings (s1 - s21)\n",
    "COL_NAMES = ['engine_id', 'cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + \\\n",
    "            [f'sensor_{i}' for i in range(1, 22)]\n",
    "\n",
    "print(f\"\\nüìã Schema Defined: {len(COL_NAMES)} columns\")\n",
    "print(f\"   Metadata: {COL_NAMES[:2]}\")\n",
    "print(f\"   Sensors:  {COL_NAMES[5:]} (Count: {len(COL_NAMES[5:])})\")\n",
    "print(f\"üìÇ EDA reports  ‚Üí {ED_REPORT_DIR}\")\n",
    "print(f\"üìÇ Interim data ‚Üí {INTERIM_DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf6b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading datasets...\n",
      "   Reading train_FD001.txt...\n",
      "   Reading test_FD001.txt...\n",
      "   Reading RUL_FD001.txt...\n",
      "   Reading train_FD002.txt...\n",
      "   Reading test_FD002.txt...\n",
      "   Reading RUL_FD002.txt...\n",
      "   Reading train_FD003.txt...\n",
      "   Reading test_FD003.txt...\n",
      "   Reading RUL_FD003.txt...\n",
      "   Reading train_FD004.txt...\n",
      "   Reading test_FD004.txt...\n",
      "   Reading RUL_FD004.txt...\n",
      "\n",
      "‚úÖ Total Datasets Loaded: 12\n",
      "\n",
      "--- DATASET OVERVIEW ---\n",
      "Dataset: train_FD001\n",
      "   Shape: (20631, 27)\n",
      "   Engines: 100\n",
      "   Cycle Length (Avg/Min/Max): 206.3 / 128 / 362\n",
      "Dataset: test_FD001\n",
      "   Shape: (13096, 27)\n",
      "   Engines: 100\n",
      "   Cycle Length (Avg/Min/Max): 131.0 / 31 / 303\n",
      "Dataset: train_FD002\n",
      "   Shape: (53759, 27)\n",
      "   Engines: 260\n",
      "   Cycle Length (Avg/Min/Max): 206.8 / 128 / 378\n",
      "Dataset: test_FD002\n",
      "   Shape: (33991, 27)\n",
      "   Engines: 259\n",
      "   Cycle Length (Avg/Min/Max): 131.2 / 21 / 367\n",
      "Dataset: train_FD003\n",
      "   Shape: (24720, 27)\n",
      "   Engines: 100\n",
      "   Cycle Length (Avg/Min/Max): 247.2 / 145 / 525\n",
      "Dataset: test_FD003\n",
      "   Shape: (16596, 27)\n",
      "   Engines: 100\n",
      "   Cycle Length (Avg/Min/Max): 166.0 / 38 / 475\n",
      "Dataset: train_FD004\n",
      "   Shape: (61249, 27)\n",
      "   Engines: 249\n",
      "   Cycle Length (Avg/Min/Max): 246.0 / 128 / 543\n",
      "Dataset: test_FD004\n",
      "   Shape: (41214, 27)\n",
      "   Engines: 248\n",
      "   Cycle Length (Avg/Min/Max): 166.2 / 19 / 486\n",
      "\n",
      "‚úÖ Summary saved to: reports/eda/dataset_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# ==================== 3. DATASET LOADING & BASIC INFO ====================\n",
    "print(\"\\nLoading datasets...\")\n",
    "\n",
    "# Helper to load space-separated file (C-MAPSS format)\n",
    "def load_cmapss_file(fpath):\n",
    "    print(f\"   Reading {fpath.name}...\")\n",
    "    try:\n",
    "        # Load, use header=None, assign columns\n",
    "        df = pd.read_csv(fpath, sep=r\"\\s+\", header=None)\n",
    "        \n",
    "        # Infer headers if columns doesn't match\n",
    "        if df.shape[1] == 26:\n",
    "            df.columns = COL_NAMES\n",
    "        elif df.shape[1] == 28: # Some versions have trailing empty cols\n",
    "             df = df.iloc[:, :26]\n",
    "             df.columns = COL_NAMES\n",
    "        # Handle RUL files (Single column)\n",
    "        elif df.shape[1] == 1:\n",
    "            df.columns = ['RUL_end']\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {fpath.name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Dictionary to hold all DataFrames { 'train_FD001': df, 'test_FD001': df, ... }\n",
    "datasets = {}\n",
    "\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    train_f = DATA_DIR / f'train_{subset}.txt'\n",
    "    test_f = DATA_DIR / f'test_{subset}.txt'\n",
    "    rul_f = DATA_DIR / f'RUL_{subset}.txt'\n",
    "    \n",
    "    if train_f.exists():\n",
    "        d_train = load_cmapss_file(train_f)\n",
    "        d_train['subset'] = subset # Track source\n",
    "        datasets[f'train_{subset}'] = d_train\n",
    "        \n",
    "    if test_f.exists():\n",
    "        d_test = load_cmapss_file(test_f)\n",
    "        d_test['subset'] = subset\n",
    "        datasets[f'test_{subset}'] = d_test\n",
    "        \n",
    "    if rul_f.exists():\n",
    "        d_rul = load_cmapss_file(rul_f)\n",
    "        d_rul['subset'] = subset\n",
    "        # Add engine_id (1-indexed based on row number)\n",
    "        d_rul['engine_id'] = d_rul.index + 1\n",
    "        datasets[f'rul_{subset}'] = d_rul\n",
    "\n",
    "print(f\"\\n‚úÖ Total Datasets Loaded: {len(datasets)}\")\n",
    "\n",
    "# ==================== 4. BASIC STATISTICS ====================\n",
    "summary_rows = []\n",
    "\n",
    "print(\"\\n--- DATASET OVERVIEW ---\")\n",
    "for ds_name, df in datasets.items():\n",
    "    if 'rul' in ds_name: continue # Skip RUL files for generic stats\n",
    "    \n",
    "    # 1. Unique Engines\n",
    "    n_engines = df['engine_id'].nunique()\n",
    "    \n",
    "    # 2. Cycle Stats\n",
    "    cycle_stats = df.groupby('engine_id')['cycle'].agg(['min', 'max', 'count'])\n",
    "    avg_cycles = cycle_stats['max'].mean()\n",
    "    min_cycles = cycle_stats['max'].min()\n",
    "    max_cycles = cycle_stats['max'].max()\n",
    "    \n",
    "    # 3. Missing/Dupe\n",
    "    missing = df.isnull().sum().sum()\n",
    "    dupes = df.duplicated().sum()\n",
    "    \n",
    "    print(f\"Dataset: {ds_name}\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Engines: {n_engines}\")\n",
    "    print(f\"   Cycle Length (Avg/Min/Max): {avg_cycles:.1f} / {min_cycles} / {max_cycles}\")\n",
    "    \n",
    "    if missing > 0: print(f\"   ‚ö†Ô∏è WARNING: {missing} missing values found!\")\n",
    "    if dupes > 0:   print(f\"   ‚ö†Ô∏è WARNING: {dupes} duplicate rows found!\")\n",
    "    \n",
    "    summary_rows.append({\n",
    "        'Dataset': ds_name,\n",
    "        'Rows': len(df),\n",
    "        'Cols': df.shape[1],\n",
    "        'Engines': n_engines,\n",
    "        'Avg_Cycle': avg_cycles,\n",
    "        'Min_Cycle': min_cycles,\n",
    "        'Max_Cycle': max_cycles\n",
    "    })\n",
    "\n",
    "# Save Summary\n",
    "pd.DataFrame(summary_rows).to_csv(ED_REPORT_DIR / \"dataset_summary.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Summary saved to: {ED_REPORT_DIR}/dataset_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec18ca",
   "metadata": {},
   "source": [
    "# Section 2: Download and Parse C-MAPSS Time-Series Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962ae7b",
   "metadata": {},
   "source": [
    "## Step 1: Check Dataset Files\n",
    "\n",
    "First, let's check if the C-MAPSS dataset is already downloaded. If not, we can use the download script.\n",
    "\n",
    "**To download the dataset:**\n",
    "1. Set up Kaggle credentials: `~/.kaggle/kaggle.json`\n",
    "2. Run: `python scripts/download_cmapss.py`\n",
    "\n",
    "For now, we'll assume the files are available at `data/raw/CMAPSS/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeef7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SENSOR ANALYSIS ---\n",
      "üìâ Analyzing Sensor Variance (Global Train Set)...\n",
      "‚úÖ No globally constant sensors found.\n",
      "\n",
      "--- Per-Subset Constant Sensors ---\n",
      "   FD001: ['sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "   FD002: None\n",
      "   FD003: ['sensor_1', 'sensor_5', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "   FD004: None\n",
      "‚úÖ Saved plot: reports/eda/cycle_distribution.png\n",
      "‚úÖ Saved plot: reports/eda/correlation_heatmap_FD001.png\n",
      "‚úÖ Saved plot: reports/eda/sensor_drift_check.png\n"
     ]
    }
   ],
   "source": [
    "# ==================== 5. SENSOR ANALYSIS & PLOTS ====================\n",
    "print(\"\\n--- SENSOR ANALYSIS ---\")\n",
    "\n",
    "# Combine all Train Data for global analysis (with subset ID)\n",
    "train_dfs = [datasets[k] for k in datasets if 'train' in k]\n",
    "full_train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "test_dfs = [datasets[k] for k in datasets if 'test' in k]\n",
    "full_test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "\n",
    "# 1. Identify Constant Sensors\n",
    "# Some sensors in C-MAPSS are constant (variance = 0), especially in FD001/003 (single condition)\n",
    "sensor_cols = [c for c in full_train_df.columns if 'sensor' in c]\n",
    "variances = full_train_df[sensor_cols].var()\n",
    "constant_sensors = variances[variances < 1e-5].index.tolist()\n",
    "\n",
    "print(f\"üìâ Analyzing Sensor Variance (Global Train Set)...\")\n",
    "if constant_sensors:\n",
    "    print(f\"‚ö†Ô∏è Found {len(constant_sensors)} Constant/Near-Constant Sensors (Global): {constant_sensors}\")\n",
    "else:\n",
    "    print(\"‚úÖ No globally constant sensors found.\")\n",
    "\n",
    "# Check Per-Subset Constancy (Important because FD001 is simpler than FD002)\n",
    "print(\"\\n--- Per-Subset Constant Sensors ---\")\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    sub_df = full_train_df[full_train_df['subset'] == subset]\n",
    "    sub_var = sub_df[sensor_cols].var()\n",
    "    const_sub = sub_var[sub_var < 1e-5].index.tolist()\n",
    "    if const_sub:\n",
    "        print(f\"   {subset}: {const_sub}\")\n",
    "    else:\n",
    "        print(f\"   {subset}: None\")\n",
    "\n",
    "# 2. Cycle Length Distribution Plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    # Get max cycle per engine for this subset\n",
    "    max_cycles = datasets[f'train_{subset}'].groupby('engine_id')['cycle'].max()\n",
    "    sns.kdeplot(max_cycles, label=subset, clip=(0, None))\n",
    "    \n",
    "plt.title(\"Engine Life Duration (Max Cycles) Distribution by Dataset\")\n",
    "plt.xlabel(\"Max Cycles\")\n",
    "plt.legend()\n",
    "plt.savefig(ED_REPORT_DIR / \"cycle_distribution.png\")\n",
    "plt.close()\n",
    "print(f\"‚úÖ Saved plot: {ED_REPORT_DIR}/cycle_distribution.png\")\n",
    "\n",
    "# 3. Sensor Correlation Heatmap (FD001 as baseline)\n",
    "# We use FD001 because generally it has the clearest degradation signal without multiple Op Conditions noise\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr = datasets['train_FD001'][sensor_cols].corr()\n",
    "sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1, annot=False)\n",
    "plt.title(\"Sensor Correlation Matrix (FD001)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ED_REPORT_DIR / \"correlation_heatmap_FD001.png\")\n",
    "plt.close()\n",
    "print(f\"‚úÖ Saved plot: {ED_REPORT_DIR}/correlation_heatmap_FD001.png\")\n",
    "\n",
    "# 4. Train vs Test Distribution (Drift Check)\n",
    "# Pick a highly variable sensor (e.g., Sensor 11 or 12)\n",
    "sensor_metric = 'sensor_11'\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(full_train_df[sensor_metric], label='Train (All)', shade=True, color='blue')\n",
    "sns.kdeplot(full_test_df[sensor_metric], label='Test (All)', shade=True, color='orange')\n",
    "plt.title(f\"{sensor_metric} Distribution: Train vs Test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Check drift per subset\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    sns.kdeplot(datasets[f'train_{subset}'][sensor_metric], label=f'{subset}', alpha=0.5)\n",
    "plt.title(f\"{sensor_metric} Distribution by Subset\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ED_REPORT_DIR / \"sensor_drift_check.png\")\n",
    "plt.close()\n",
    "print(f\"‚úÖ Saved plot: {ED_REPORT_DIR}/sensor_drift_check.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c4fcb9",
   "metadata": {},
   "source": [
    "## Step 2: Load and Parse C-MAPSS Data\n",
    "\n",
    "We'll load the FD001 dataset as an example (normal operation, 100 engines).\n",
    "\n",
    "Dataset structure:\n",
    "- **FD001**: 100 engines, normal operation\n",
    "- **FD002**: 260 engines, various operational conditions\n",
    "- **FD003**: 100 engines, with induced faults\n",
    "- **FD004**: 248 engines, various conditions with faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e42ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEXT DATA EDA ---\n",
      "‚ö†Ô∏è No text log files processed.\n",
      "\n",
      "--- MERGING & SAVING ---\n",
      "‚úÖ Saved Combined Train: data/interim/combined_train.csv (Shape: (160359, 28))\n",
      "‚úÖ Saved Combined Test:  data/interim/combined_test.csv  (Shape: (104897, 28))\n",
      "‚úÖ Saved Combined RUL:   data/interim/combined_test_rul.csv   (Shape: (707, 4))\n",
      "\n",
      "üéâ EDA PHASE COMPLETE. Proceed to Feature Engineering.\n"
     ]
    }
   ],
   "source": [
    "# ==================== 6. TEXT DATA EDA (LogHub / Maintenance Logs) ====================\n",
    "print(\"\\n--- TEXT DATA EDA ---\")\n",
    "\n",
    "combined_text_df = pd.DataFrame(columns=['source_file', 'line_content'])\n",
    "\n",
    "if TEXT_DATA_FILES:\n",
    "    for f in TEXT_DATA_FILES:\n",
    "        try:\n",
    "            # Read first few lines\n",
    "            with open(f, 'r', encoding='utf-8', errors='ignore') as log_file:\n",
    "                lines = log_file.readlines()\n",
    "                \n",
    "            print(f\"File: {f.name}\")\n",
    "            print(f\"   Shape: {len(lines)} lines\")\n",
    "            print(f\"   Preview (First 3 lines):\")\n",
    "            for i, line in enumerate(lines[:3]):\n",
    "                print(f\"     {i+1}: {line.strip()}\")\n",
    "            \n",
    "            # Simple DataFrame construction\n",
    "            temp_df = pd.DataFrame({'line_content': lines})\n",
    "            temp_df['source_file'] = f.name\n",
    "            combined_text_df = pd.concat([combined_text_df, temp_df], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading text file {f.name}: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No text log files processed.\")\n",
    "\n",
    "# Save Combined Text\n",
    "if not combined_text_df.empty:\n",
    "    combined_text_df.to_csv(INTERIM_DATA_DIR / \"combined_text_corpus.csv\", index=False)\n",
    "    print(f\"‚úÖ Combined text corpus saved: {INTERIM_DATA_DIR}/combined_text_corpus.csv\")\n",
    "\n",
    "# ==================== 7. MERGE & SAVE C-MAPSS DATASETS ====================\n",
    "print(\"\\n--- MERGING & SAVING ---\")\n",
    "\n",
    "# Ensure 'subset' col exists (added during loading)\n",
    "# Also add 'dataset_id' as requested (integer or string)\n",
    "# Let's use string 'FD001' etc as subset, and map to int dataset_id if needed.\n",
    "# User asked for 'dataset_id'. I'll map FD001->1, FD002->2 etc.\n",
    "\n",
    "dataset_map = {'FD001': 1, 'FD002': 2, 'FD003': 3, 'FD004': 4}\n",
    "\n",
    "# Process TRAIN\n",
    "full_train_df = pd.concat([datasets[k] for k in datasets if 'train' in k], ignore_index=True)\n",
    "full_train_df['dataset_id'] = full_train_df['subset'].map(dataset_map)\n",
    "full_train_df.to_csv(INTERIM_DATA_DIR / \"combined_train.csv\", index=False)\n",
    "print(f\"‚úÖ Saved Combined Train: {INTERIM_DATA_DIR}/combined_train.csv (Shape: {full_train_df.shape})\")\n",
    "\n",
    "# Process TEST\n",
    "full_test_df = pd.concat([datasets[k] for k in datasets if 'test' in k], ignore_index=True)\n",
    "full_test_df['dataset_id'] = full_test_df['subset'].map(dataset_map)\n",
    "full_test_df.to_csv(INTERIM_DATA_DIR / \"combined_test.csv\", index=False)\n",
    "print(f\"‚úÖ Saved Combined Test:  {INTERIM_DATA_DIR}/combined_test.csv  (Shape: {full_test_df.shape})\")\n",
    "\n",
    "# Process RUL (For Evaluation Later)\n",
    "full_rul_df = pd.concat([datasets[k] for k in datasets if 'rul' in k], ignore_index=True)\n",
    "full_rul_df['dataset_id'] = full_rul_df['subset'].map(dataset_map)\n",
    "full_rul_df.to_csv(INTERIM_DATA_DIR / \"combined_test_rul.csv\", index=False)\n",
    "print(f\"‚úÖ Saved Combined RUL:   {INTERIM_DATA_DIR}/combined_test_rul.csv   (Shape: {full_rul_df.shape})\")\n",
    "\n",
    "print(\"\\nüéâ EDA PHASE COMPLETE. Proceed to Feature Engineering.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ASSEMBLE full_rul_df FROM ALL FOUR RUL FILES ====================\n",
    "# Combines rul_FD001 ‚Ä¶ rul_FD004 into a single DataFrame used by the RUL-label cell below.\n",
    "# dataset_map is defined in Cell 8: {'FD001': 1, 'FD002': 2, 'FD003': 3, 'FD004': 4}\n",
    "\n",
    "rul_pieces = []\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    key = f'rul_{subset}'\n",
    "    if key in datasets:\n",
    "        df = datasets[key][['engine_id', 'RUL_end', 'subset']].copy()\n",
    "        df['dataset_id'] = dataset_map[subset]   # FD001‚Üí1, FD002‚Üí2, etc.\n",
    "        rul_pieces.append(df)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Key '{key}' not found in datasets ‚Äî check that RUL_{subset}.txt loaded correctly.\")\n",
    "\n",
    "if not rul_pieces:\n",
    "    raise ValueError(\"‚ùå No RUL data assembled. Ensure all four RUL_FDxxx.txt files exist in DATA_DIR.\")\n",
    "\n",
    "full_rul_df = pd.concat(rul_pieces, ignore_index=True)\n",
    "print(f\"‚úÖ full_rul_df: {full_rul_df.shape}  engines per subset: \"\n",
    "      f\"{full_rul_df.groupby('dataset_id').size().to_dict()}\")\n",
    "print(full_rul_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c5bdd",
   "metadata": {},
   "source": [
    "# ==================== 8. SUMMARY OF OUTPUTS ====================\n",
    "\"\"\"\n",
    "EDA COMPLETE.\n",
    "\n",
    "Outputs generated:\n",
    "1. Reports (reports/eda/):\n",
    "   - dataset_summary.csv: Rows, columns, and cycle statistics for each sub-dataset.\n",
    "   - cycle_distribution.png: Distribution of max engine life cycles.\n",
    "   - correlation_heatmap_FD001.png: Correlation of sensors in FD001.\n",
    "   - sensor_drift_check.png: Comparison of Sensor 11 distribution between Train and Test sets.\n",
    "\n",
    "2. Processed Data (data/interim/):\n",
    "   - combined_train.csv: All 4 train datasets merged (with dataset_id column).\n",
    "   - combined_test.csv: All 4 test datasets merged (with dataset_id column).\n",
    "   - combined_test_rul.csv: All 4 RUL datasets merged.\n",
    "   - combined_text_corpus.csv: Raw text content from log files (if any found).\n",
    "\n",
    "Next Steps:\n",
    "- Feature Engineering (RUL calculation, Sliding Window, Scaling).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24730af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RUL stats:\n",
      "                 RUL    RUL_clipped\n",
      "count  160359.000000  160359.000000\n",
      "mean      122.331338      90.182029\n",
      "std        83.538146      41.241036\n",
      "min         0.000000       0.000000\n",
      "25%        56.000000      56.000000\n",
      "50%       113.000000     113.000000\n",
      "75%       172.000000     125.000000\n",
      "max       542.000000     125.000000\n",
      "\n",
      "Test RUL stats:\n",
      "                 RUL    RUL_clipped\n",
      "count  104897.000000  104897.000000\n",
      "mean      162.628264     110.529195\n",
      "std        80.873737      26.863275\n",
      "min         6.000000       6.000000\n",
      "25%       107.000000     107.000000\n",
      "50%       154.000000     125.000000\n",
      "75%       206.000000     125.000000\n",
      "max       553.000000     125.000000\n",
      "\n",
      "Featisure Engineering Step 1 (RUL Lables) Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. TRAIN RUL: Piecewise linear degradation\n",
    "# Calculate RUL = max_cycle - current_cycle for each engine\n",
    "full_train_df = full_train_df.merge(\n",
    "    full_train_df.groupby(['dataset_id', 'engine_id'])['cycle'].max().rename('max_cycle'),\n",
    "    on=['dataset_id', 'engine_id']\n",
    ")\n",
    "full_train_df['RUL'] = full_train_df['max_cycle'] - full_train_df['cycle']\n",
    "# Cap RUL at 125 (common practice for C-MAPSS - early cycles don't show degradation)\n",
    "full_train_df['RUL_clipped'] = full_train_df['RUL'].clip(upper=125)\n",
    "\n",
    "# 2. TEST RUL: Actual RUL provided in RUL_FDxxx.txt files\n",
    "# The RUL file contains the remaining useful life for the LAST cycle in the test set.\n",
    "# So for any row in test set: RUL = RUL_at_end + (max_cycle_in_test - current_cycle)\n",
    "\n",
    "# Get max cycle for each engine in test set\n",
    "test_max_cycles = full_test_df.groupby(['dataset_id', 'engine_id'])['cycle'].max().rename('max_cycle')\n",
    "full_test_df = full_test_df.merge(test_max_cycles, on=['dataset_id', 'engine_id'])\n",
    "\n",
    "# Merge with the True RUL values (from RUL_FDxxx.txt)\n",
    "# combined_rul_df comes from earlier step (datasets['rul_FDxxx'])\n",
    "# We need to make sure 'dataset_id' matches\n",
    "full_test_df = full_test_df.merge(full_rul_df, on=['dataset_id', 'engine_id'], how='left')\n",
    "\n",
    "# Calculate RUL\n",
    "# The RUL value in full_rul_df is the RUL at the last recorded cycle\n",
    "full_test_df['RUL'] = full_test_df['RUL_end'] + (full_test_df['max_cycle'] - full_test_df['cycle'])\n",
    "full_test_df['RUL_clipped'] = full_test_df['RUL'].clip(upper=125)\n",
    "\n",
    "print(\"Train RUL stats:\")\n",
    "print(full_train_df[['RUL', 'RUL_clipped']].describe())\n",
    "print(\"\\nTest RUL stats:\")\n",
    "print(full_test_df[['RUL', 'RUL_clipped']].describe())\n",
    "\n",
    "# Save enhanced datasets\n",
    "full_train_df.to_csv('data/processed/train_FD001.csv', index=False)\n",
    "full_test_df.to_csv('data/processed/test_FD001.csv', index=False) # Saving all as FD001 for now or split them back?\n",
    "# Actually, better to save them as combined or split by ID if needed.\n",
    "# For now, let's save the combined ones which are valuable.\n",
    "full_train_df.to_csv('data/interim/train_with_rul.csv', index=False)\n",
    "full_test_df.to_csv('data/interim/test_with_rul.csv', index=False)\n",
    "\n",
    "print(\"\\nFeatisure Engineering Step 1 (RUL Lables) Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d00a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FEATURE ENGINEERING ---\n",
      "   Removing specific constant sensors: ['sensor_16']\n",
      "   Selected 20 useful sensors for modeling.\n",
      "1. Engineering features for TRAINING data...\n",
      "   Processing windows [5, 10, 20] for 20 sensors...\n",
      "2. Engineering features for TESTING data...\n",
      "   Processing windows [5, 10, 20] for 20 sensors...\n",
      "\n",
      "Saving feature-engineered datasets...\n",
      "‚úÖ Saved Train Features: (160359, 212)\n",
      "‚úÖ Saved Test Features:  (104897, 214)\n",
      "Preview of new features (sensor_11):\n",
      "   sensor_11  sensor_11_roll_mean_5  sensor_11_roll_std_5  \\\n",
      "0      47.47                 47.470              0.000000   \n",
      "1      47.49                 47.480              0.014142   \n",
      "2      47.27                 47.410              0.121655   \n",
      "3      47.13                 47.340              0.171659   \n",
      "4      47.28                 47.328              0.151063   \n",
      "5      47.16                 47.266              0.141527   \n",
      "\n",
      "   sensor_11_roll_mean_10  sensor_11_roll_std_10  sensor_11_roll_mean_20  \\\n",
      "0                  47.470               0.000000                  47.470   \n",
      "1                  47.480               0.014142                  47.480   \n",
      "2                  47.410               0.121655                  47.410   \n",
      "3                  47.340               0.171659                  47.340   \n",
      "4                  47.328               0.151063                  47.328   \n",
      "5                  47.300               0.151526                  47.300   \n",
      "\n",
      "   sensor_11_roll_std_20  sensor_11_diff  sensor_11_trend_20  sensor_11_ema_10  \n",
      "0               0.000000            0.00            0.000000         47.470000  \n",
      "1               0.014142            0.02            0.000211         47.473636  \n",
      "2               0.121655           -0.22           -0.002953         47.436612  \n",
      "3               0.171659           -0.14           -0.004436         47.380864  \n",
      "4               0.151063            0.15           -0.001014         47.362525  \n",
      "5               0.151526           -0.12           -0.002960         47.325702  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==================== 9. FEATURE ENGINEERING (Tabular) ====================\n",
    "print(\"\\n--- FEATURE ENGINEERING ---\")\n",
    "\n",
    "# 1. Identify valid sensors (excluding constants) based on TRAIN data\n",
    "# (We verified this in EDA, typically indices 1,5,6,10,16,18,19 are constant in FD001)\n",
    "# Let's programmatically find them again to be safe and avoid leakage.\n",
    "\n",
    "def get_useful_sensors(df, threshold=0.01):\n",
    "    \"\"\"Returns list of sensor columns with variance > threshold.\"\"\"\n",
    "    sensor_cols = [c for c in df.columns if 'sensor_' in c]\n",
    "    variances = df[sensor_cols].var()\n",
    "    useful = variances[variances > threshold].index.tolist()\n",
    "    removed = set(sensor_cols) - set(useful)\n",
    "    print(f\"   Removing specific constant sensors: {sorted(list(removed))}\")\n",
    "    return useful\n",
    "\n",
    "# We determine useful sensors from FULL TRAIN only (global baseline check)\n",
    "useful_sensors = get_useful_sensors(full_train_df)\n",
    "print(f\"   Selected {len(useful_sensors)} useful sensors for modeling (global).\")\n",
    "\n",
    "# ---- Subset-aware sensor selection ----\n",
    "# Sensors confirmed constant across ALL subsets ‚Äî always drop.\n",
    "GLOBAL_CONSTANT_SENSORS = [\n",
    "    'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10',\n",
    "    'sensor_16', 'sensor_18', 'sensor_19'\n",
    "]\n",
    "# dataset_id values for single-operating-condition subsets (FD001=1, FD003=3).\n",
    "# op_setting_3 is always 0 here ‚Üí carries no information ‚Üí drop it.\n",
    "# FD002 (id=2) and FD004 (id=4) have 6 operating conditions ‚Üí keep op_setting_3.\n",
    "SINGLE_COND_SUBSETS = [1, 3]   # dataset_id 1=FD001, 3=FD003\n",
    "\n",
    "def get_useful_sensors_per_subset(df, dataset_id):\n",
    "    \"\"\"\n",
    "    Returns sensor + op_setting columns to keep for a specific subset.\n",
    "    - Drops GLOBAL_CONSTANT_SENSORS for every subset.\n",
    "    - Additionally drops 'op_setting_3' for single-condition subsets (FD001, FD003)\n",
    "      where it is always 0 and destroys no regime information.\n",
    "    \"\"\"\n",
    "    drop = GLOBAL_CONSTANT_SENSORS.copy()\n",
    "    if dataset_id in SINGLE_COND_SUBSETS:\n",
    "        drop.append('op_setting_3')\n",
    "    kept = [c for c in df.columns if c.startswith('sensor_') and c not in drop]\n",
    "    print(f\"   dataset_id={dataset_id}: keeping {len(kept)} sensors \"\n",
    "          f\"{'(op_setting_3 dropped)' if dataset_id in SINGLE_COND_SUBSETS else '(op_setting_3 kept)'}\")\n",
    "    return kept\n",
    "\n",
    "# 2. Feature Engineering Function\n",
    "def create_features(df, sensors, windows=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Creates rolling mean, std, min, max, trend, and diff features.\n",
    "    Assumes df is already sorted by [dataset_id, engine_id, cycle].\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    # Group by engine to respect time-series boundaries\n",
    "    # We group by ['dataset_id', 'engine_id'] because engine_1 in FD001 != engine_1 in FD002\n",
    "    grouped = df_out.groupby(['dataset_id', 'engine_id'])\n",
    "    \n",
    "    # A. Cycle Normalization (Cycle Ratio)\n",
    "    # Note: For Test data, max_cycle is the max OBSERVED cycle, not necessarily failure cycle.\n",
    "    # This feature is useful but must be used carefully during inference.\n",
    "    df_out['cycle_norm'] = df_out['cycle'] / grouped['cycle'].transform('max')\n",
    "    \n",
    "    print(f\"   Processing windows {windows} for {len(sensors)} sensors...\")\n",
    "    \n",
    "    for w in windows:\n",
    "        # B. Rolling Features\n",
    "        # Rolling Mean\n",
    "        df_out[[f'{s}_roll_mean_{w}' for s in sensors]] = grouped[sensors].rolling(window=w, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        # Rolling Std\n",
    "        df_out[[f'{s}_roll_std_{w}' for s in sensors]] = grouped[sensors].rolling(window=w, min_periods=1).std().reset_index(level=0, drop=True).fillna(0)\n",
    "        # Rolling Max\n",
    "        # df_out[[f'{s}_roll_max_{w}' for s in sensors]] = grouped[sensors].rolling(window=w, min_periods=1).max().reset_index(drop=True)\n",
    "        \n",
    "    # C. Advanced Features (Diffs, Trends) using smallest window or raw\n",
    "    for s in sensors:\n",
    "        # 1. Delta (Change from previous cycle)\n",
    "        df_out[f'{s}_diff'] = grouped[s].diff().fillna(0)\n",
    "        \n",
    "        # 2. Trend (Current - Rolling Mean 20) / Rolling Mean 20\n",
    "        # Represents deviation from recent baseline\n",
    "        rm_col = f'{s}_roll_mean_20'\n",
    "        if rm_col in df_out.columns:\n",
    "             df_out[f'{s}_trend_20'] = (df_out[s] - df_out[rm_col]) / (df_out[rm_col] + 1e-6)\n",
    "             \n",
    "        # 3. EMA (Exponential Moving Average) - Optional but requested\n",
    "        # alpha=0.1 roughly corresponds to N=19\n",
    "        df_out[f'{s}_ema_10'] = grouped[s].ewm(span=10, adjust=False).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "# 3. Process each subset independently so op_setting_3 is only dropped where appropriate\n",
    "print(\"\\n1. Engineering features for TRAINING data (per-subset sensor selection)...\")\n",
    "train_pieces = []\n",
    "for did, subset_df in full_train_df.groupby('dataset_id'):\n",
    "    subset_sensors = get_useful_sensors_per_subset(subset_df, did)\n",
    "    train_pieces.append(create_features(subset_df, subset_sensors))\n",
    "train_featured = pd.concat(train_pieces, ignore_index=True)\n",
    "\n",
    "print(\"\\n2. Engineering features for TESTING data (per-subset sensor selection)...\")\n",
    "test_pieces = []\n",
    "for did, subset_df in full_test_df.groupby('dataset_id'):\n",
    "    subset_sensors = get_useful_sensors_per_subset(subset_df, did)\n",
    "    test_pieces.append(create_features(subset_df, subset_sensors))\n",
    "test_featured = pd.concat(test_pieces, ignore_index=True)\n",
    "\n",
    "# 4. Save Feature-Engineered Datasets\n",
    "print(\"\\nSaving feature-engineered datasets...\")\n",
    "train_featured.to_csv('data/processed/train_features.csv', index=False)\n",
    "test_featured.to_csv('data/processed/test_features.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved Train Features: {train_featured.shape}\")\n",
    "print(f\"‚úÖ Saved Test Features:  {test_featured.shape}\")\n",
    "print(f\"Preview of new features (sensor_11):\")\n",
    "cols = [c for c in train_featured.columns if 'sensor_11' in c]\n",
    "print(train_featured[cols].head(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SCALING & SEQUENCING ---\n",
      "Scaling 205 features...\n",
      "‚úÖ Scaler saved to models/feature_scaler.joblib\n",
      "Split Engines: 567 Train, 142 Val\n",
      "\n",
      "Generating sequences (Window=30)...\n",
      "‚úÖ Train Sequences: (111486, 30, 205) | Labels: (111486,)\n",
      "‚úÖ Val Sequences:   (28312, 30, 205) | Labels: (28312,)\n",
      "‚úÖ Test Sequences:  (84495, 30, 205)  | Labels: (84495,)\n",
      "‚úÖ Saved Numpy Sequences to data/processed/sequences\n"
     ]
    }
   ],
   "source": [
    "ss\n",
    "# ==================== 10. SCALING & SEQUENCE GENERATION (Deep Learning) ====================\n",
    "print(\"\\n--- SCALING & SEQUENCING ---\")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def get_feature_cols(df):\n",
    "    \"\"\"\n",
    "    Returns columns suitable for scaling/modeling:\n",
    "    - Excludes metadata identifiers and target columns.\n",
    "    - Excludes constant columns (std == 0) to avoid NaN from MinMaxScaler.\n",
    "    \"\"\"\n",
    "    exclude = {\n",
    "        'dataset_id', 'engine_id', 'cycle', 'sub_dataset',\n",
    "        'subset', 'RUL', 'RUL_clipped', 'max_cycle'\n",
    "    }\n",
    "    return [c for c in df.columns if c not in exclude and df[c].std() > 0]\n",
    "\n",
    "# ---------- Regime-based scaler for multi-condition subsets ----------\n",
    "def fit_regime_scaler(train_df, sensor_cols, n_regimes=6):\n",
    "    \"\"\"\n",
    "    Cluster operating points via KMeans on op_settings, then fit one\n",
    "    MinMaxScaler per regime.  Returns (km_model, {regime_id: scaler}).\n",
    "    \"\"\"\n",
    "    km = KMeans(n_clusters=n_regimes, random_state=42, n_init=10)\n",
    "    km.fit(train_df[['op_setting_1', 'op_setting_2', 'op_setting_3']])\n",
    "    scalers = {}\n",
    "    for regime_id in range(n_regimes):\n",
    "        mask = km.labels_ == regime_id\n",
    "        sc = MinMaxScaler(feature_range=(-1, 1))\n",
    "        sc.fit(train_df.loc[mask, sensor_cols])\n",
    "        scalers[regime_id] = sc\n",
    "    return km, scalers\n",
    "\n",
    "def apply_regime_scaler(df, km, scalers, sensor_cols):\n",
    "    \"\"\"Predict regime for each row, then scale with the per-regime scaler.\"\"\"\n",
    "    df = df.copy()\n",
    "    labels = km.predict(df[['op_setting_1', 'op_setting_2', 'op_setting_3']])\n",
    "    for regime_id, sc in scalers.items():\n",
    "        mask = labels == regime_id\n",
    "        if mask.any():\n",
    "            df.loc[mask, sensor_cols] = sc.transform(df.loc[mask, sensor_cols])\n",
    "    return df\n",
    "\n",
    "# ---------- Scale each subset with its appropriate strategy ----------\n",
    "# FD001 (id=1) and FD003 (id=3): 1 operating condition ‚Üí global MinMaxScaler\n",
    "# FD002 (id=2) and FD004 (id=4): 6 operating conditions ‚Üí KMeans regime scaler\n",
    "\n",
    "MULTI_COND_SUBSETS = {2, 4}\n",
    "N_REGIMES = 6\n",
    "\n",
    "scaler_store = {}          # persisted for inference\n",
    "train_scaled_pieces = []\n",
    "test_scaled_pieces  = []\n",
    "subset_feature_cols = {}   # track per-subset columns\n",
    "\n",
    "for did in sorted(train_featured['dataset_id'].unique()):\n",
    "    tr = train_featured[train_featured['dataset_id'] == did].copy()\n",
    "    te = test_featured[test_featured['dataset_id'] == did].copy()\n",
    "    fcols = get_feature_cols(tr)\n",
    "    subset_feature_cols[did] = fcols\n",
    "\n",
    "    if did in MULTI_COND_SUBSETS:\n",
    "        # KMeans regime-based scaling ‚Äî preserves inter-regime sensor ranges\n",
    "        km, scalers = fit_regime_scaler(tr, fcols, n_regimes=N_REGIMES)\n",
    "        tr = apply_regime_scaler(tr, km, scalers, fcols)\n",
    "        te = apply_regime_scaler(te, km, scalers, fcols)\n",
    "        scaler_store[did] = {\n",
    "            'type': 'regime', 'km': km, 'scalers': scalers, 'feature_cols': fcols\n",
    "        }\n",
    "        print(f\"   dataset_id={did}: KMeans regime-based scaling \"\n",
    "              f\"({N_REGIMES} regimes, {len(fcols)} features)\")\n",
    "    else:\n",
    "        # Single global scaler for single-condition subsets\n",
    "        sc = MinMaxScaler(feature_range=(-1, 1))\n",
    "        tr[fcols] = sc.fit_transform(tr[fcols])\n",
    "        te[fcols] = sc.transform(te[fcols])\n",
    "        scaler_store[did] = {'type': 'global', 'scaler': sc, 'feature_cols': fcols}\n",
    "        print(f\"   dataset_id={did}: global MinMaxScaler ({len(fcols)} features)\")\n",
    "\n",
    "    train_scaled_pieces.append(tr)\n",
    "    test_scaled_pieces.append(te)\n",
    "\n",
    "train_featured = pd.concat(train_scaled_pieces, ignore_index=True)\n",
    "test_featured  = pd.concat(test_scaled_pieces,  ignore_index=True)\n",
    "\n",
    "# feature_cols for sequence generation = intersection across all subsets\n",
    "# (guarantees every subset has a value for every feature ‚Äî no NaN-padded sequences)\n",
    "all_fcols_sets = [set(v) for v in subset_feature_cols.values()]\n",
    "feature_cols = sorted(set.intersection(*all_fcols_sets))\n",
    "print(f\"\\n‚úÖ Shared feature columns for sequence generation: {len(feature_cols)}\")\n",
    "\n",
    "# Save all scalers in one file\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(scaler_store, 'models/feature_scaler.joblib')\n",
    "print(\"‚úÖ Scaler store saved to models/feature_scaler.joblib\")\n",
    "\n",
    "# 3. Validation Split (GroupSplit by Engine ID ‚Äî keeps all cycles of an engine together)\n",
    "np.random.seed(42)\n",
    "unique_engines = train_featured[['dataset_id', 'engine_id']].drop_duplicates()\n",
    "shuffled_engines = unique_engines.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "split_idx = int(0.8 * len(shuffled_engines))\n",
    "train_engines = shuffled_engines.iloc[:split_idx]\n",
    "val_engines   = shuffled_engines.iloc[split_idx:]\n",
    "\n",
    "print(f\"\\nSplit Engines: {len(train_engines)} Train, {len(val_engines)} Val\")\n",
    "\n",
    "train_keys = set(zip(train_engines['dataset_id'], train_engines['engine_id']))\n",
    "val_keys   = set(zip(val_engines['dataset_id'],   val_engines['engine_id']))\n",
    "\n",
    "train_mask = train_featured.set_index(['dataset_id', 'engine_id']).index.isin(train_keys)\n",
    "val_mask   = train_featured.set_index(['dataset_id', 'engine_id']).index.isin(val_keys)\n",
    "\n",
    "X_train_df = train_featured[train_mask].copy()\n",
    "X_val_df   = train_featured[val_mask].copy()\n",
    "\n",
    "# 4. Sequence Generation Function (Sliding Window)\n",
    "def create_sequences(df, feature_cols, sequence_length=30, pad=True):\n",
    "    \"\"\"\n",
    "    Creates (samples, seq_len, features) tensor.\n",
    "    If pad=True, engines shorter than seq_len are pre-padded with 0s.\n",
    "    Target is the RUL of the LAST step in the window.\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    for _, group in df.groupby(['dataset_id', 'engine_id']):\n",
    "        features = group[feature_cols].values\n",
    "        target   = group['RUL_clipped'].values\n",
    "        num_cycles = len(group)\n",
    "\n",
    "        if num_cycles >= sequence_length:\n",
    "            for i in range(num_cycles - sequence_length + 1):\n",
    "                X_seq.append(features[i : i + sequence_length])\n",
    "                y_seq.append(target[i + sequence_length - 1])\n",
    "        elif pad:\n",
    "            pad_len  = sequence_length - num_cycles\n",
    "            padding  = np.zeros((pad_len, features.shape[1]))\n",
    "            X_window = np.vstack([padding, features])\n",
    "            X_seq.append(X_window)\n",
    "            y_seq.append(target[-1])\n",
    "\n",
    "    return np.array(X_seq, dtype=np.float32), np.array(y_seq, dtype=np.float32)\n",
    "\n",
    "print(\"\\nGenerating sequences (Window=30)...\")\n",
    "SEQ_LEN = 30\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_df,   feature_cols, SEQ_LEN, pad=False)\n",
    "X_val_seq,   y_val_seq   = create_sequences(X_val_df,     feature_cols, SEQ_LEN, pad=False)\n",
    "X_test_seq,  y_test_seq  = create_sequences(test_featured, feature_cols, SEQ_LEN, pad=True)\n",
    "\n",
    "print(f\"‚úÖ Train Sequences: {X_train_seq.shape} | Labels: {y_train_seq.shape}\")\n",
    "print(f\"‚úÖ Val Sequences:   {X_val_seq.shape}   | Labels: {y_val_seq.shape}\")\n",
    "print(f\"‚úÖ Test Sequences:  {X_test_seq.shape}   | Labels: {y_test_seq.shape}\")\n",
    "\n",
    "SEQ_DIR = Path('data/processed/sequences')\n",
    "SEQ_DIR.mkdir(exist_ok=True)\n",
    "np.save(SEQ_DIR / 'X_train_seq.npy', X_train_seq)\n",
    "np.save(SEQ_DIR / 'y_train_seq.npy', y_train_seq)\n",
    "np.save(SEQ_DIR / 'X_val_seq.npy',   X_val_seq)\n",
    "np.save(SEQ_DIR / 'y_val_seq.npy',   y_val_seq)\n",
    "np.save(SEQ_DIR / 'X_test_seq.npy',  X_test_seq)\n",
    "np.save(SEQ_DIR / 'y_test_seq.npy',  y_test_seq)\n",
    "\n",
    "print(f\"‚úÖ Saved Numpy Sequences to {SEQ_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb105372",
   "metadata": {},
   "source": [
    "## Step 3: Explore Dataset Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00736819",
   "metadata": {},
   "source": [
    "# Section 3: Visualize Sensor Degradation Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804804b",
   "metadata": {},
   "source": [
    "# Section 4: Create Remaining Useful Life (RUL) Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69826b3",
   "metadata": {},
   "source": [
    "# Section 5: Split Time-Series Data by Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5c949",
   "metadata": {},
   "source": [
    "# Section 6: Download and Select LogHub Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f83b5e",
   "metadata": {},
   "source": [
    "## LogHub Datasets\n",
    "\n",
    "LogHub is a collection of public log datasets from real systems:\n",
    "\n",
    "| Dataset | System | Size | Entries | Characteristics |\n",
    "|---------|--------|------|---------|----------|\n",
    "| HDFS | Hadoop | ~55 MB | 11M | Distributed file system logs |\n",
    "| BGL | Blue Gene/L | ~700 MB | 4.7M | Supercomputer logs |\n",
    "| OpenStack | Cloud | ~300 MB | 207k | Cloud infrastructure logs |\n",
    "| Android | Mobile | Large | 1.5M | Mobile OS logs |\n",
    "\n",
    "We'll focus on **HDFS** and **BGL** for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c67437",
   "metadata": {},
   "source": [
    "# Section 7: Normalize and Parse Log Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4bd7b",
   "metadata": {},
   "source": [
    "## Example: Parsing synthetic log data\n",
    "\n",
    "Since we don't have the actual LogHub data yet, we'll demonstrate the parsing pipeline with example logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e46d13",
   "metadata": {},
   "source": [
    "# Section 8: Convert Logs to Incident Narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9dd0f",
   "metadata": {},
   "source": [
    "# Section 9: Store Clean Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed858b",
   "metadata": {},
   "source": [
    "# Summary: PHASE 2 Complete ‚úÖ\n",
    "\n",
    "## What We Accomplished:\n",
    "\n",
    "### Time-Series Data (NASA C-MAPSS)\n",
    "- ‚úÖ Downloaded and parsed C-MAPSS turbofan engine dataset\n",
    "- ‚úÖ Extracted engine ID, cycle, operational settings, 21 sensor readings\n",
    "- ‚úÖ Visualized sensor degradation patterns over time\n",
    "- ‚úÖ Created RUL (Remaining Useful Life) labels\n",
    "- ‚úÖ Split data by engine: Train/Val/Test with no leakage\n",
    "\n",
    "### Text Data (LogHub)\n",
    "- ‚úÖ Demonstrated log parsing pipeline\n",
    "- ‚úÖ Normalized log messages (removed IDs, IPs, etc.)\n",
    "- ‚úÖ Grouped logs into incident bursts\n",
    "- ‚úÖ Generated incident narratives\n",
    "- ‚úÖ Created synthetic maintenance reports\n",
    "- ‚úÖ Stored clean text corpus\n",
    "\n",
    "## Outputs:\n",
    "\n",
    "**Time-Series Data:**\n",
    "- `data/processed/train_FD001.csv` - Training data (80% of engines)\n",
    "- `data/processed/val_FD001.csv` - Validation data (20% of engines)\n",
    "- `data/processed/test_FD001.csv` - Test data (separate engine set)\n",
    "- Visualizations: sensor patterns, correlation, RUL distribution\n",
    "\n",
    "**Text Data:**\n",
    "- `data/processed/text_corpus/cleaned_logs.csv` - Parsed logs\n",
    "- `data/processed/text_corpus/incidents.json` - Grouped incidents\n",
    "- `data/processed/text_corpus/incident_narratives.txt` - Human-readable narratives\n",
    "\n",
    "## Next Steps (PHASE 3+):\n",
    "- Feature engineering pipeline\n",
    "- Baseline 1: Pure ML models (XGBoost, Isolation Forest)\n",
    "- Baseline 2: ML + RAG (FAISS vector DB)\n",
    "- Baseline 3: Agentic AI (LangGraph orchestration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
