{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35039d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project Root: /Users/xe/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures\n",
      "‚úÖ Device: mps\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 0: SETUP & CONFIGURATION ====================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "SEQ_LEN = 30\n",
    "BATCH_SIZE = 256\n",
    "CLIP_MAX = 125\n",
    "ID_COL = \"engine_id_global\" # Global unique ID across subsets\n",
    "TARGET = \"RUL_clip\"\n",
    "TIME_COL = \"cycle\"\n",
    "\n",
    "# Setup Project Root\n",
    "try:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    while not (PROJECT_ROOT / 'src').exists() and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "except:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Define Paths\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"CMAPSS\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "SUBSETS = [\"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [PROCESSED_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('notebook')\n",
    "print(f\"‚úÖ Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úÖ Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9d3bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:notebook:üöÄ Loading C-MAPSS Data with Strict Validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All 12/12 C-MAPSS files verified.\n",
      "‚úÖ Loaded Train: (160359, 28), Test: (104897, 28)\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 1: DATA LOADING (STRICT) ====================\n",
    "def load_data():\n",
    "    logger.info(\"üöÄ Loading C-MAPSS Data with Strict Validation...\")\n",
    "    col_names = ['engine_id', 'cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + \\\n",
    "                [f'sensor_{i}' for i in range(1, 22)]\n",
    "    \n",
    "    list_train, list_test = [], []\n",
    "    \n",
    "    # 1. Validation Loop\n",
    "    missing_files = []\n",
    "    for subset in SUBSETS:\n",
    "        for ftype in [\"train\", \"test\", \"RUL\"]:\n",
    "            fname = f\"{ftype}_{subset}.txt\"\n",
    "            if not (DATA_DIR / fname).exists():\n",
    "                missing_files.append(fname)\n",
    "                \n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"‚ùå CRITICAL: Missing required files: {missing_files}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All 12/12 C-MAPSS files verified.\")\n",
    "\n",
    "    # 2. Loading Loop (Robust)\n",
    "    for subset in SUBSETS:\n",
    "        # Load Train\n",
    "        df_tr = pd.read_csv(DATA_DIR / f\"train_{subset}.txt\", sep=r\"\\s+\", header=None)\n",
    "        df_tr = df_tr.iloc[:, :len(col_names)] # STRICT SLICING\n",
    "        df_tr.columns = col_names\n",
    "        \n",
    "        # Load Test\n",
    "        df_te = pd.read_csv(DATA_DIR / f\"test_{subset}.txt\", sep=r\"\\s+\", header=None)\n",
    "        df_te = df_te.iloc[:, :len(col_names)] # STRICT SLICING\n",
    "        df_te.columns = col_names\n",
    "        \n",
    "        # Create Unique Global IDs (subset + engine_id)\n",
    "        df_tr[ID_COL] = f\"{subset}_\" + df_tr['engine_id'].astype(str)\n",
    "        df_te[ID_COL] = f\"{subset}_\" + df_te['engine_id'].astype(str)\n",
    "        \n",
    "        df_tr['dataset_id'] = subset\n",
    "        df_te['dataset_id'] = subset\n",
    "        \n",
    "        list_train.append(df_tr)\n",
    "        list_test.append(df_te)\n",
    "        \n",
    "    return pd.concat(list_train, ignore_index=True), pd.concat(list_test, ignore_index=True)\n",
    "\n",
    "train_all, test_all = load_data()\n",
    "print(f\"‚úÖ Loaded Train: {train_all.shape}, Test: {test_all.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4626203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:notebook:üöÄ Creating RUL Labels with Strict ID Mapping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Labels Created and Verified.\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 2: RUL LABEL CREATION (FIXED) ====================\n",
    "def create_labels(train_df, test_df):\n",
    "    logger.info(\"üöÄ Creating RUL Labels with Strict ID Mapping...\")\n",
    "    \n",
    "    # 1. Train RUL (Linear degradation)\n",
    "    max_cycle = train_df.groupby(ID_COL)[TIME_COL].max().rename('max_cycle')\n",
    "    train_df = train_df.merge(max_cycle, on=ID_COL)\n",
    "    train_df['RUL'] = train_df['max_cycle'] - train_df[TIME_COL]\n",
    "    train_df[TARGET] = train_df['RUL'].clip(upper=CLIP_MAX)\n",
    "    \n",
    "    # 2. Test RUL (Using Ground Truth Files)\n",
    "    rul_list = []\n",
    "    for subset in SUBSETS:\n",
    "        # Robust Reading\n",
    "        try:\n",
    "            df_rul = pd.read_csv(DATA_DIR / f\"RUL_{subset}.txt\", sep=r'\\s+', header=None, names=['RUL_end'])\n",
    "        except:\n",
    "             df_rul = pd.read_csv(DATA_DIR / f\"RUL_{subset}.txt\", header=None, names=['RUL_end'])\n",
    "        \n",
    "        # Robust ID Creation (Using explicit Index)\n",
    "        df_rul['dataset_id'] = subset\n",
    "        df_rul['engine_id_index'] = df_rul.index + 1\n",
    "        df_rul[ID_COL] = df_rul['dataset_id'] + \"_\" + df_rul['engine_id_index'].astype(str)\n",
    "        \n",
    "        rul_list.append(df_rul[[ID_COL, 'RUL_end']])\n",
    "        \n",
    "    all_ruls = pd.concat(rul_list, ignore_index=True)\n",
    "    test_df = test_df.merge(all_ruls, on=ID_COL, how='left')\n",
    "    \n",
    "    # Check for unmapped RULs\n",
    "    missing_ruls = test_df['RUL_end'].isnull().sum()\n",
    "    if missing_ruls > 0:\n",
    "        raise ValueError(f\"‚ùå CRITICAL: Failed to map {missing_ruls} test engines to RUL file!\")\n",
    "        \n",
    "    # Calculate RUL at each time step for Test\n",
    "    max_cycle_test = test_df.groupby(ID_COL)[TIME_COL].max().rename('max_cycle')\n",
    "    test_df = test_df.merge(max_cycle_test, on=ID_COL)\n",
    "    test_df['RUL'] = test_df['RUL_end'] + (test_df['max_cycle'] - test_df[TIME_COL])\n",
    "    test_df[TARGET] = test_df['RUL'].clip(upper=CLIP_MAX)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "full_train_df, full_test_df = create_labels(train_all, test_all)\n",
    "print(\"‚úÖ Labels Created and Verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d01ab07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:notebook:üöÄ Feature Engineering (Sorted & Leakage-Free)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features Selected (24): ['op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21']\n",
      "‚úÖ Data Sorted by Engine and Time.\n",
      "  ‚úÖ FD001: global scaling\n",
      "  ‚úÖ FD002: per-regime scaling (6 regimes)\n",
      "  ‚úÖ FD003: global scaling\n",
      "  ‚úÖ FD004: per-regime scaling (6 regimes)\n",
      "‚úÖ Per-subset scalers saved.\n",
      "‚è≥ Creating Sequences...\n",
      "‚úÖ Saved X_train, X_val, X_test, metadata to /Users/xe/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/data/processed\n",
      "‚úÖ Sequences: Train (112876, 30, 24), Val (26922, 30, 24), Test Last (690, 30, 24)\n",
      "‚úÖ Per-subset engines in last-cycle test: {'FD001': 100, 'FD002': 253, 'FD003': 100, 'FD004': 237}\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 3: FEATURE ENGINEERING (SORTED & FIXED) ====================\n",
    "def process_data_pipeline(full_train, full_test):\n",
    "    logger.info(\"üöÄ Feature Engineering (Sorted & Leakage-Free)...\")\n",
    "    \n",
    "    # --- 1. Feature Selection (CRITICAL FIX) ---\n",
    "    meta_cols = [ID_COL, TIME_COL, 'RUL', TARGET, 'max_cycle', 'RUL_end', 'dataset_id', 'engine_id', 'label_fail', 'engine_id_index']\n",
    "    available_cols = [c for c in full_train.columns if c not in meta_cols]\n",
    "    print(f\"‚úÖ Features Selected ({len(available_cols)}): {available_cols}\")\n",
    "    \n",
    "    # Save Feature List\n",
    "    import json\n",
    "    with open(MODELS_DIR / 'features.json', 'w') as f:\n",
    "        json.dump(available_cols, f)\n",
    "    \n",
    "    # --- 2. Train/Val Split (by Engine) ---\n",
    "    unique_engines = full_train[ID_COL].unique()\n",
    "    train_ids, val_ids = train_test_split(unique_engines, test_size=0.2, random_state=RANDOM_SEED)\n",
    "    \n",
    "    train_df = full_train[full_train[ID_COL].isin(train_ids)].copy()\n",
    "    val_df = full_train[full_train[ID_COL].isin(val_ids)].copy()\n",
    "    \n",
    "    # SORTING CRITICAL for Sliding Window\n",
    "    train_df = train_df.sort_values([ID_COL, TIME_COL])\n",
    "    val_df = val_df.sort_values([ID_COL, TIME_COL])\n",
    "    full_test = full_test.sort_values([ID_COL, TIME_COL])\n",
    "    \n",
    "    print(f\"‚úÖ Data Sorted by Engine and Time.\")\n",
    "    \n",
    "    # --- 3. Scaling (per-subset, with per-regime normalization for FD002/FD004) ---\n",
    "    # FD001/FD003 have 1 operating condition ‚Üí global scaling is fine.\n",
    "    # FD002/FD004 have 6 operating regimes ‚Üí per-regime scaling prevents\n",
    "    # blending regime differences with degradation signal.\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    MULTI_COND = {'FD002', 'FD004'}\n",
    "    N_REGIMES = 6\n",
    "    op_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3']\n",
    "    \n",
    "    scaled_train_pieces, scaled_val_pieces, scaled_test_pieces = [], [], []\n",
    "    scalers_dict = {}   # for optional persistence\n",
    "    \n",
    "    for subset in SUBSETS:\n",
    "        tr = train_df[train_df['dataset_id'] == subset].copy()\n",
    "        va = val_df[val_df['dataset_id'] == subset].copy()\n",
    "        te = full_test[full_test['dataset_id'] == subset].copy()\n",
    "        \n",
    "        if subset in MULTI_COND:\n",
    "            # Cluster operating regimes on training op_settings\n",
    "            km = KMeans(n_clusters=N_REGIMES, random_state=42, n_init=10)\n",
    "            km.fit(tr[op_cols])\n",
    "            \n",
    "            tr_labels = km.predict(tr[op_cols])\n",
    "            va_labels = km.predict(va[op_cols]) if len(va) > 0 else np.array([])\n",
    "            te_labels = km.predict(te[op_cols]) if len(te) > 0 else np.array([])\n",
    "            \n",
    "            # Scale per-regime: fit on train, transform val/test with same scaler\n",
    "            regime_scalers = {}\n",
    "            for r in range(N_REGIMES):\n",
    "                tr_mask = tr_labels == r\n",
    "                if tr_mask.any():\n",
    "                    sc_r = StandardScaler()\n",
    "                    tr.loc[tr.index[tr_mask], available_cols] = sc_r.fit_transform(\n",
    "                        tr.loc[tr.index[tr_mask], available_cols]\n",
    "                    )\n",
    "                    regime_scalers[r] = sc_r\n",
    "                    \n",
    "                    if len(va_labels) > 0:\n",
    "                        va_mask = va_labels == r\n",
    "                        if va_mask.any():\n",
    "                            va.loc[va.index[va_mask], available_cols] = sc_r.transform(\n",
    "                                va.loc[va.index[va_mask], available_cols]\n",
    "                            )\n",
    "                    if len(te_labels) > 0:\n",
    "                        te_mask = te_labels == r\n",
    "                        if te_mask.any():\n",
    "                            te.loc[te.index[te_mask], available_cols] = sc_r.transform(\n",
    "                                te.loc[te.index[te_mask], available_cols]\n",
    "                            )\n",
    "            scalers_dict[subset] = {'kmeans': km, 'regime_scalers': regime_scalers}\n",
    "            print(f\"  ‚úÖ {subset}: per-regime scaling ({N_REGIMES} regimes)\")\n",
    "        else:\n",
    "            # Single operating condition ‚Äî global scaling\n",
    "            sc = StandardScaler()\n",
    "            tr[available_cols] = sc.fit_transform(tr[available_cols])\n",
    "            if len(va) > 0:\n",
    "                va[available_cols] = sc.transform(va[available_cols])\n",
    "            te[available_cols] = sc.transform(te[available_cols])\n",
    "            scalers_dict[subset] = {'scaler': sc}\n",
    "            print(f\"  ‚úÖ {subset}: global scaling\")\n",
    "        \n",
    "        scaled_train_pieces.append(tr)\n",
    "        scaled_val_pieces.append(va)\n",
    "        scaled_test_pieces.append(te)\n",
    "    \n",
    "    train_df = pd.concat(scaled_train_pieces, ignore_index=True)\n",
    "    val_df   = pd.concat(scaled_val_pieces, ignore_index=True)\n",
    "    full_test = pd.concat(scaled_test_pieces, ignore_index=True)\n",
    "    \n",
    "    # Save Scalers\n",
    "    try:\n",
    "        joblib.dump(scalers_dict, MODELS_DIR / 'scalers_per_subset.joblib')\n",
    "        print(\"‚úÖ Per-subset scalers saved.\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # --- 4. Sliding Window Sequence Generation ---\n",
    "    def create_sequences(df):\n",
    "        X, y = [], []\n",
    "        engine_uids, time_cycles = [], []\n",
    "        \n",
    "        for engine_id, engine_data in df.groupby(ID_COL, sort=False):\n",
    "            if len(engine_data) < SEQ_LEN: continue\n",
    "            \n",
    "            vals = engine_data[available_cols].values\n",
    "            targs = engine_data[TARGET].values\n",
    "            cycles = engine_data[TIME_COL].values\n",
    "            \n",
    "            for i in range(len(engine_data) - SEQ_LEN + 1):\n",
    "                X.append(vals[i : i + SEQ_LEN])\n",
    "                y.append(targs[i + SEQ_LEN - 1])\n",
    "                engine_uids.append(engine_id)\n",
    "                time_cycles.append(cycles[i + SEQ_LEN - 1])\n",
    "                \n",
    "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32), np.array(engine_uids), np.array(time_cycles)\n",
    "\n",
    "    print(\"‚è≥ Creating Sequences...\")\n",
    "    X_train, y_train, _, _ = create_sequences(train_df)\n",
    "    X_val, y_val, _, _ = create_sequences(val_df)\n",
    "    \n",
    "    X_test_all, y_test_all, test_engine_ids, test_cycles = create_sequences(full_test)\n",
    "\n",
    "    # Save processed data for Phase 4 & 5\n",
    "    np.save(PROCESSED_DIR / 'X_train.npy', X_train)\n",
    "    np.save(PROCESSED_DIR / 'y_train.npy', y_train)\n",
    "    np.save(PROCESSED_DIR / 'X_val.npy', X_val)\n",
    "    np.save(PROCESSED_DIR / 'y_val.npy', y_val)\n",
    "    np.save(PROCESSED_DIR / 'X_test.npy', X_test_all)\n",
    "    np.save(PROCESSED_DIR / 'y_test.npy', y_test_all)\n",
    "    \n",
    "    # SAVE METADATA (CRITICAL FOR LIFECYCLE PLOT)\n",
    "    np.save(PROCESSED_DIR / 'test_engine_uid.npy', test_engine_ids)\n",
    "    np.save(PROCESSED_DIR / 'test_cycle_end.npy', test_cycles)\n",
    "    print(f\"‚úÖ Saved X_train, X_val, X_test, metadata to {PROCESSED_DIR}\")\n",
    "\n",
    "    # Test Data Last: last window per engine + per-engine subset tag for per-subset eval\n",
    "    X_test_last, y_test_last, test_engine_ids_last = [], [], []\n",
    "    for engine_id, engine_data in full_test.groupby(ID_COL, sort=False):\n",
    "        if len(engine_data) >= SEQ_LEN:\n",
    "            X_test_last.append(engine_data[available_cols].values[-SEQ_LEN:])\n",
    "            y_test_last.append(engine_data[TARGET].values[-1])\n",
    "            # Extract subset tag e.g. \"FD001\" from \"FD001_1\"\n",
    "            test_engine_ids_last.append(engine_id.split('_')[0])\n",
    "            \n",
    "    X_test_last = np.array(X_test_last, dtype=np.float32)\n",
    "    y_test_last = np.array(y_test_last, dtype=np.float32)\n",
    "    test_engine_ids_last = np.array(test_engine_ids_last)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test_last, y_test_last, available_cols, test_engine_ids_last\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test_last, y_test_last, features, test_engine_ids_last = process_data_pipeline(full_train_df, full_test_df)\n",
    "print(f\"‚úÖ Sequences: Train {X_train.shape}, Val {X_val.shape}, Test Last {X_test_last.shape}\")\n",
    "print(f\"‚úÖ Per-subset engines in last-cycle test: { {s: int((test_engine_ids_last==s).sum()) for s in ['FD001','FD002','FD003','FD004']} }\")\n",
    "\n",
    "# Prepare Flattened Data for Baselines (Take last step of sequence)\n",
    "X_train_flat = X_train[:, -1, :]\n",
    "X_val_flat = X_val[:, -1, :]\n",
    "X_test_last_flat = X_test_last[:, -1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858a5e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training Baseline Models...\n",
      "   Training Random Forest...\n",
      "   ‚úÖ Random Forest        | Train: 7.10 | Val: 20.71 | Test: 19.27\n",
      "   Training XGBoost...\n",
      "   ‚úÖ XGBoost              | Train: 16.65 | Val: 20.71 | Test: 19.13\n",
      "   Training Gradient Boosting...\n",
      "   ‚úÖ Gradient Boosting    | Train: 19.05 | Val: 20.91 | Test: 19.48\n",
      "‚úÖ Saved XGBoost Model.\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 4: BASELINE MODELS (RF, GB, XGB) ====================\n",
    "print(\"\\nüöÄ Training Baseline Models...\")\n",
    "baseline_results = {}\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=RANDOM_SEED),\n",
    "    \"XGBoost\": xgb.XGBRegressor(n_estimators=100, n_jobs=-1, random_state=RANDOM_SEED),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"   Training {name}...\")\n",
    "    model.fit(X_train_flat, y_train)\n",
    "    \n",
    "    # Predict Train\n",
    "    y_pred_train = model.predict(X_train_flat)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    \n",
    "    # Predict Val\n",
    "    y_pred_val = model.predict(X_val_flat)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "    \n",
    "    # Predict Test\n",
    "    y_pred_test = model.predict(X_test_last_flat)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_last, y_pred_test))\n",
    "    \n",
    "    baseline_results[name] = {\"Train RMSE\": rmse_train, \"Val RMSE\": rmse_val, \"Test RMSE\": rmse_test}\n",
    "    print(f\"   ‚úÖ {name:20} | Train: {rmse_train:.2f} | Val: {rmse_val:.2f} | Test: {rmse_test:.2f}\")\n",
    "\n",
    "# Save Best Model Logic (e.g. XGBoost)\n",
    "try:\n",
    "    best_xgb = models[\"XGBoost\"]\n",
    "    joblib.dump(best_xgb, MODELS_DIR / \"xgb_model.joblib\")\n",
    "    print(\"‚úÖ Saved XGBoost Model.\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1b6064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training LSTM...\n",
      "   Config: epochs=60, lr=0.001, grad_clip=1.0, scheduler_patience=5, early_stop=30\n",
      "   Epoch 01/60 | Train Loss: 5438.95 | Val Loss: 2812.49 | LR: 0.001000 | Best: 2812.49\n",
      "   Epoch 02/60 | Train Loss: 1558.60 | Val Loss: 564.98 | LR: 0.001000 | Best: 564.98\n",
      "   Epoch 03/60 | Train Loss: 307.99 | Val Loss: 252.42 | LR: 0.001000 | Best: 252.42\n",
      "   Epoch 04/60 | Train Loss: 197.09 | Val Loss: 241.50 | LR: 0.001000 | Best: 241.50\n",
      "   Epoch 05/60 | Train Loss: 178.58 | Val Loss: 237.72 | LR: 0.001000 | Best: 237.72\n",
      "   Epoch 10/60 | Train Loss: 118.33 | Val Loss: 283.11 | LR: 0.001000 | Best: 237.72\n",
      "   Epoch 15/60 | Train Loss: 65.95 | Val Loss: 313.10 | LR: 0.000500 | Best: 237.72\n",
      "   Epoch 20/60 | Train Loss: 43.10 | Val Loss: 343.89 | LR: 0.000250 | Best: 237.72\n",
      "   Epoch 25/60 | Train Loss: 34.12 | Val Loss: 355.69 | LR: 0.000125 | Best: 237.72\n",
      "   Epoch 30/60 | Train Loss: 29.90 | Val Loss: 354.84 | LR: 0.000063 | Best: 237.72\n",
      "   Epoch 35/60 | Train Loss: 28.08 | Val Loss: 359.62 | LR: 0.000031 | Best: 237.72\n",
      "   ‚èπÔ∏è Early stopping at epoch 35 (no improvement for 30 epochs)\n",
      "   ‚úÖ Training complete. Best Val Loss: 237.72\n",
      "‚úÖ LSTM Final Metrics: Train RMSE: 12.88 | Val RMSE: 15.32 | Test RMSE: 18.55\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 6A: LSTM TRAINING (IMPROVED) ====================\n",
    "print(\"\\nüöÄ Training LSTM...\")\n",
    "\n",
    "# Data Loaders\n",
    "lstm_train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "lstm_val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "\n",
    "train_loader = DataLoader(lstm_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(lstm_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class LSTMRULModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=100, num_layers=2, output_dim=1):\n",
    "        super(LSTMRULModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # NOTE: No output activation ‚Äî ReLU was removed because it floors\n",
    "        # predictions at 0, biasing gradients and inflating residuals (~60 cycles).\n",
    "        # Targets are already clipped to [0, 125]; MSE loss is sufficient.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out_last = out[:, -1, :]\n",
    "        return self.fc(out_last)\n",
    "\n",
    "lstm = LSTMRULModel(input_dim=X_train.shape[2]).to(device)\n",
    "opt = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "# --- Training improvements ---\n",
    "# 1. LR Scheduler: halve LR when val loss plateaus for 5 epochs\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5)\n",
    "# 2. Gradient clipping max norm\n",
    "GRAD_CLIP = 1.0\n",
    "# 3. Early stopping patience\n",
    "EARLY_STOP_PATIENCE = 30\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "EPOCHS = 60\n",
    "\n",
    "print(f\"   Config: epochs={EPOCHS}, lr=0.001, grad_clip={GRAD_CLIP}, \"\n",
    "      f\"scheduler_patience=5, early_stop={EARLY_STOP_PATIENCE}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    lstm.train()\n",
    "    train_loss = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device).unsqueeze(1)\n",
    "        opt.zero_grad()\n",
    "        loss = crit(lstm(x_batch), y_batch)\n",
    "        loss.backward()\n",
    "        # Gradient clipping to prevent exploding gradients in LSTM\n",
    "        torch.nn.utils.clip_grad_norm_(lstm.parameters(), GRAD_CLIP)\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    lstm.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xv_batch, yv_batch in val_loader:\n",
    "            xv_batch, yv_batch = xv_batch.to(device), yv_batch.to(device).unsqueeze(1)\n",
    "            val_loss += crit(lstm(xv_batch), yv_batch).item()\n",
    "            \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    # Step the LR scheduler based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = opt.param_groups[0]['lr']\n",
    "    \n",
    "    # Save Best Model + Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(lstm.state_dict(), MODELS_DIR / \"lstm_best.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0 or epochs_no_improve == 0:\n",
    "        print(f\"   Epoch {epoch+1:02d}/{EPOCHS} | Train Loss: {train_loss:.2f} | Val Loss: {val_loss:.2f} | LR: {current_lr:.6f} | Best: {best_val_loss:.2f}\")\n",
    "    \n",
    "    if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"   ‚èπÔ∏è Early stopping at epoch {epoch+1} (no improvement for {EARLY_STOP_PATIENCE} epochs)\")\n",
    "        break\n",
    "\n",
    "print(f\"   ‚úÖ Training complete. Best Val Loss: {best_val_loss:.2f}\")\n",
    "\n",
    "# Final Eval Metrics\n",
    "lstm.load_state_dict(torch.load(MODELS_DIR / \"lstm_best.pth\", map_location=device, weights_only=True))\n",
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    # Train Eval ‚Äî batched to avoid OOM\n",
    "    y_pred_train_list = []\n",
    "    temp_train_loader = DataLoader(lstm_train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    for x_b, _ in temp_train_loader:\n",
    "        y_pred_train_list.append(lstm(x_b.to(device)).cpu())\n",
    "    y_pred_train = torch.cat(y_pred_train_list).numpy().flatten()\n",
    "    lstm_rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "\n",
    "    # Val Eval\n",
    "    y_pred_val_list = []\n",
    "    for x_b, _ in val_loader:\n",
    "        y_pred_val_list.append(lstm(x_b.to(device)).cpu())\n",
    "    y_pred_val = torch.cat(y_pred_val_list).numpy().flatten()\n",
    "    lstm_rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "\n",
    "    # Test Eval (Last window per engine)\n",
    "    y_pred_test = lstm(torch.FloatTensor(X_test_last).to(device)).cpu().numpy().flatten()\n",
    "    lstm_rmse_test = np.sqrt(mean_squared_error(y_test_last, y_pred_test))\n",
    "    \n",
    "    print(f\"‚úÖ LSTM Final Metrics: Train RMSE: {lstm_rmse_train:.2f} | Val RMSE: {lstm_rmse_val:.2f} | Test RMSE: {lstm_rmse_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ea535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training TCN...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m crit = nn.MSELoss()\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# --- Training improvements (same as LSTM) ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m scheduler_tcn = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_tcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m GRAD_CLIP_TCN = \u001b[32m1.0\u001b[39m\n\u001b[32m     38\u001b[39m EARLY_STOP_PATIENCE_TCN = \u001b[32m12\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 6B: TCN TRAINING (IMPROVED) ====================\n",
    "print(\"\\nüöÄ Training TCN...\")\n",
    "\n",
    "# 1. Define TCN Data Loaders (Isolated from LSTM)\n",
    "tcn_train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)), \n",
    "                         batch_size=BATCH_SIZE, shuffle=True)\n",
    "tcn_val_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val)), \n",
    "                         batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TCNModel, self).__init__()\n",
    "        # Simple TCN-like structure with 1D Convs\n",
    "        # ReLU is used in HIDDEN layers only (fine for feature extraction).\n",
    "        # Final layer is Linear(32,1) with NO output activation.\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv1d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1), # Global Pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Seq, Feat) -> (Batch, Feat, Seq) for Conv1d\n",
    "        return self.net(x.transpose(1, 2))\n",
    "\n",
    "tcn = TCNModel(input_dim=X_train.shape[2]).to(device)\n",
    "opt_tcn = torch.optim.Adam(tcn.parameters(), lr=0.001)\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "# --- Training improvements (same as LSTM) ---\n",
    "scheduler_tcn = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_tcn, mode='min', factor=0.5, patience=5)\n",
    "GRAD_CLIP_TCN = 1.0\n",
    "EARLY_STOP_PATIENCE_TCN = 12\n",
    "\n",
    "best_val_loss_tcn = float('inf')\n",
    "epochs_no_improve_tcn = 0\n",
    "EPOCHS = 60\n",
    "\n",
    "print(f\"   Config: epochs={EPOCHS}, lr=0.001, grad_clip={GRAD_CLIP_TCN}, \"\n",
    "      f\"scheduler_patience=5, early_stop={EARLY_STOP_PATIENCE_TCN}\")\n",
    "print(f\"   Training on {len(tcn_train_loader)} batches...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tcn.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # CRITICAL: Use tcn_train_loader, NOT train_loader\n",
    "    for x_batch, y_batch in tcn_train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device).unsqueeze(1)\n",
    "        opt_tcn.zero_grad()\n",
    "        loss = crit(tcn(x_batch), y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(tcn.parameters(), GRAD_CLIP_TCN)\n",
    "        opt_tcn.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    tcn.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xv_batch, yv_batch in tcn_val_loader:\n",
    "            xv_batch, yv_batch = xv_batch.to(device), yv_batch.to(device).unsqueeze(1)\n",
    "            val_loss += crit(tcn(xv_batch), yv_batch).item()\n",
    "            \n",
    "    train_loss /= len(tcn_train_loader)\n",
    "    val_loss /= len(tcn_val_loader)\n",
    "    \n",
    "    # Step the LR scheduler\n",
    "    scheduler_tcn.step(val_loss)\n",
    "    current_lr = opt_tcn.param_groups[0]['lr']\n",
    "    \n",
    "    # Save Best Model + Early Stopping\n",
    "    if val_loss < best_val_loss_tcn:\n",
    "        best_val_loss_tcn = val_loss\n",
    "        epochs_no_improve_tcn = 0\n",
    "        torch.save(tcn.state_dict(), MODELS_DIR / \"tcn_best.pth\")\n",
    "    else:\n",
    "        epochs_no_improve_tcn += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0 or epochs_no_improve_tcn == 0:\n",
    "        print(f\"   Epoch {epoch+1:02d}/{EPOCHS} | Train Loss: {train_loss:.2f} | Val Loss: {val_loss:.2f} | LR: {current_lr:.6f} | Best: {best_val_loss_tcn:.2f}\")\n",
    "    \n",
    "    if epochs_no_improve_tcn >= EARLY_STOP_PATIENCE_TCN:\n",
    "        print(f\"   ‚èπÔ∏è Early stopping at epoch {epoch+1} (no improvement for {EARLY_STOP_PATIENCE_TCN} epochs)\")\n",
    "        break\n",
    "\n",
    "print(f\"   ‚úÖ Training complete. Best Val Loss: {best_val_loss_tcn:.2f}\")\n",
    "\n",
    "# Final Eval Metrics\n",
    "tcn.load_state_dict(torch.load(MODELS_DIR / \"tcn_best.pth\", map_location=device, weights_only=True))\n",
    "tcn.eval()\n",
    "with torch.no_grad():\n",
    "    # Train Eval\n",
    "    y_pred_train_list = []\n",
    "    temp_tcn_train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    for x_b, _ in temp_tcn_train_loader:\n",
    "        y_pred_train_list.append(tcn(x_b.to(device)).cpu())\n",
    "    y_pred_train = torch.cat(y_pred_train_list).numpy().flatten()\n",
    "    tcn_rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "\n",
    "    # Val Eval\n",
    "    y_pred_val_list = []\n",
    "    for x_b, _ in tcn_val_loader:\n",
    "        y_pred_val_list.append(tcn(x_b.to(device)).cpu())\n",
    "    y_pred_val = torch.cat(y_pred_val_list).numpy().flatten()\n",
    "    tcn_rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "\n",
    "    # Test Eval (Last window per engine)\n",
    "    y_pred_test = tcn(torch.FloatTensor(X_test_last).to(device)).cpu().numpy().flatten()\n",
    "    tcn_rmse_test = np.sqrt(mean_squared_error(y_test_last, y_pred_test))\n",
    "    \n",
    "    print(f\"‚úÖ TCN Final Metrics: Train RMSE: {tcn_rmse_train:.2f} | Val RMSE: {tcn_rmse_val:.2f} | Test RMSE: {tcn_rmse_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä FINAL SUMMARY (RMSE - Lower is Better)\n",
      "-----------------------------------------------------------------\n",
      "Model                | Train      | Val        | Test (Last Cycle)\n",
      "-----------------------------------------------------------------\n",
      "Random Forest        | 6.95       | 20.36      | 18.39     \n",
      "XGBoost              | 16.97      | 20.44      | 18.71     \n",
      "Gradient Boosting    | 20.09      | 21.56      | 19.75     \n",
      "LSTM                 | 19.96      | 22.66      | 33.28     \n",
      "TCN                  | 21.19      | 22.90      | 22.80     \n",
      "\n",
      "=========================================================================================================\n",
      "Model                | FD001 RMSE | FD002 RMSE | FD003 RMSE | FD004 RMSE |   NASA Score\n",
      "=========================================================================================================\n",
      "Random Forest        |      17.23 |      17.30 |      19.06 |      19.66 |       6781.2\n",
      "XGBoost              |      17.21 |      17.63 |      19.42 |      20.08 |       7809.9\n",
      "Gradient Boosting    |      16.64 |      18.83 |      20.51 |      21.51 |       7326.8\n",
      "LSTM                 |      34.29 |      30.94 |      35.41 |      34.30 |      64709.1\n",
      "TCN                  |      21.77 |      20.91 |      22.13 |      25.29 |      21984.7\n",
      "=========================================================================================================\n",
      "NASA Score: lower is better  |  late predictions penalised 10x harder than early ones\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================== STEP 7: FINAL COMPARISON ====================\n",
    "\n",
    "# --- NASA Score: C-MAPSS asymmetric benchmark metric ---\n",
    "# Penalty for late predictions (d>0) uses exp(d/10); early (d<0) uses exp(-d/13)\n",
    "# Lower is better.\n",
    "def nasa_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred, float) - np.asarray(y_true, float)\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d / 13) - 1, np.exp(d / 10) - 1)))\n",
    "\n",
    "# --- Wrap PyTorch models with a sklearn-compatible .predict() ---\n",
    "class TorchPredictor:\n",
    "    \"\"\"Thin wrapper so LSTM/TCN can share the same predict() interface.\"\"\"\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.FloatTensor(X).to(self.device)).cpu().numpy().flatten()\n",
    "\n",
    "# --- Build unified model registry ---\n",
    "# Baselines use X_test_last_flat (2-D); Torch models use X_test_last (3-D)\n",
    "all_models_flat = dict(models)   # RF, XGBoost, GradientBoosting (already trained)\n",
    "all_models_seq  = {}             # LSTM, TCN\n",
    "if 'lstm' in vars() and lstm is not None:\n",
    "    try:    all_models_seq['LSTM'] = TorchPredictor(lstm, device)\n",
    "    except: pass\n",
    "if 'tcn' in vars() and tcn is not None:\n",
    "    try:    all_models_seq['TCN'] = TorchPredictor(tcn, device)\n",
    "    except: pass\n",
    "\n",
    "# --- Overall RMSE summary table ---\n",
    "print('\\nüìä FINAL SUMMARY (RMSE - Lower is Better)')\n",
    "print('-' * 65)\n",
    "print(f\"{'Model':<20} | {'Train':<10} | {'Val':<10} | {'Test (Last Cycle)':<10}\")\n",
    "print('-' * 65)\n",
    "\n",
    "for m in baseline_results:\n",
    "    print(f\"{m:<20} | {baseline_results[m]['Train RMSE']:<10.2f} | {baseline_results[m]['Val RMSE']:<10.2f} | {baseline_results[m]['Test RMSE']:<10.2f}\")\n",
    "\n",
    "def get_metric(prefix, metric_type):\n",
    "    var_names = [f'{prefix}_rmse_{metric_type}', f'{prefix}_{metric_type}_rmse']\n",
    "    for v in var_names:\n",
    "        if v in globals():\n",
    "            return globals()[v]\n",
    "    return None\n",
    "\n",
    "l_train = get_metric('lstm', 'train'); l_val = get_metric('lstm', 'val'); l_test = get_metric('lstm', 'test')\n",
    "t_train = get_metric('tcn',  'train'); t_val = get_metric('tcn',  'val'); t_test = get_metric('tcn',  'test')\n",
    "\n",
    "if l_train is not None:\n",
    "    print(f\"{'LSTM':<20} | {l_train:<10.2f} | {l_val:<10.2f} | {l_test:<10.2f}\")\n",
    "else:\n",
    "    print(f\"{'LSTM':<20} | {'N/A':<10} | {'N/A':<10} | {'N/A':<10}\")\n",
    "if t_train is not None:\n",
    "    print(f\"{'TCN':<20} | {t_train:<10.2f} | {t_val:<10.2f} | {t_test:<10.2f}\")\n",
    "else:\n",
    "    print(f\"{'TCN':<20} | {'N/A':<10} | {'N/A':<10} | {'N/A':<10}\")\n",
    "\n",
    "# --- Per-subset RMSE + NASA Score table ---\n",
    "print('\\n' + '=' * 105)\n",
    "print(f\"{'Model':<20} | {'FD001 RMSE':>10} | {'FD002 RMSE':>10} | {'FD003 RMSE':>10} | {'FD004 RMSE':>10} | {'NASA Score':>12}\")\n",
    "print('=' * 105)\n",
    "\n",
    "def _eval_subset_table(model_dict, X_input):\n",
    "    for mname, model in model_dict.items():\n",
    "        row = f'{mname:<20}'\n",
    "        for sub in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "            mask = test_engine_ids_last == sub\n",
    "            if mask.sum() == 0:\n",
    "                row += f\" | {'N/A':>10}\"\n",
    "                continue\n",
    "            y_pred_sub = model.predict(X_input[mask])\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_last[mask], y_pred_sub))\n",
    "            row += f' | {rmse:>10.2f}'\n",
    "        y_pred_all = model.predict(X_input)\n",
    "        row += f' | {nasa_score(y_test_last, y_pred_all):>12.1f}'\n",
    "        print(row)\n",
    "\n",
    "_eval_subset_table(all_models_flat, X_test_last_flat)\n",
    "_eval_subset_table(all_models_seq,  X_test_last)\n",
    "print('=' * 105)\n",
    "print('NASA Score: lower is better  |  late predictions penalised 10x harder than early ones')\n",
    "\n",
    "# --- Bar chart ---\n",
    "models_final = list(baseline_results.keys())\n",
    "val_scores   = [baseline_results[m]['Val RMSE']  for m in baseline_results]\n",
    "test_scores  = [baseline_results[m]['Test RMSE'] for m in baseline_results]\n",
    "\n",
    "if l_val is not None:\n",
    "    models_final.append('LSTM'); val_scores.append(l_val); test_scores.append(l_test)\n",
    "if t_val is not None:\n",
    "    models_final.append('TCN');  val_scores.append(t_val); test_scores.append(t_test)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(models_final))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, val_scores,  width, label='Val RMSE')\n",
    "plt.bar(x + width/2, test_scores, width, label='Test RMSE (Last Cycle)')\n",
    "plt.xticks(x, models_final)\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Model Benchmarking (Fixed: No Leakage)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
