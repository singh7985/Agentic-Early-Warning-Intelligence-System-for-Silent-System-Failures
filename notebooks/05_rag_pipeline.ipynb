{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c9d31b",
   "metadata": {},
   "source": [
    "# PHASE 6 — RAG Pipeline Integration\n",
    "\n",
    "**Objectives:**\n",
    "- Build knowledge base from PHASE 5 degradation data\n",
    "- Implement document chunking and embedding\n",
    "- Create FAISS vector store for similarity search\n",
    "- Retrieve similar historical failures with citations\n",
    "- Validate retrieval relevance\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- Knowledge base with 100-300 failure incidents\n",
    "- Similarity search with >70% top-5 recall\n",
    "- Citation tracking for all retrieved incidents\n",
    "- Query: \"Find past incidents similar to current sensor deviation pattern\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5ad6a",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Imports\n",
    "\n",
    "Import all RAG modules and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f286e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-27 02:33:22 - root - INFO - Logging configured. Level: INFO, File: logs/ewis.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All modules imported successfully (source reloaded)\n",
      "✓ torch.get_num_threads() = 1\n",
      "✓ Subsets: ['FD001', 'FD002', 'FD003', 'FD004']\n",
      "✓ Notebook ready for PHASE 6: RAG Pipeline Integration\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# MUST be first — set thread / parallelism env vars BEFORE any import\n",
    "# (prevents OpenMP / MKL / tokenizers fork-deadlock in Jupyter kernels)\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['HF_HUB_OFFLINE']        = '1'\n",
    "os.environ['TRANSFORMERS_OFFLINE']   = '1'\n",
    "os.environ['OMP_NUM_THREADS']        = '1'\n",
    "os.environ['MKL_NUM_THREADS']        = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS']   = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'   # macOS Accelerate\n",
    "os.environ['NUMEXPR_MAX_THREADS']    = '1'\n",
    "\n",
    "# Standard library\n",
    "import importlib\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Ensure project root is on sys.path ──────────────────────────────\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Limit PyTorch threads (belt-and-suspenders after env vars)\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "\n",
    "# ── Force-reload edited source modules so changes take effect ────────\n",
    "import src.features.sliding_windows as _sw_mod;   importlib.reload(_sw_mod)\n",
    "import src.features.feature_selection as _fs_mod;  importlib.reload(_fs_mod)\n",
    "import src.features.pipeline as _fp_mod;           importlib.reload(_fp_mod)\n",
    "import src.rag.knowledge_base as _kb_mod;          importlib.reload(_kb_mod)\n",
    "import src.rag;                                    importlib.reload(src.rag)\n",
    "\n",
    "# RAG pipeline\n",
    "from src.rag import (\n",
    "    DocumentChunker,\n",
    "    Embedder,\n",
    "    VectorStore,\n",
    "    Retriever,\n",
    "    KnowledgeBase\n",
    ")\n",
    "from src.rag.knowledge_base import create_test_cases_from_degradation\n",
    "\n",
    "# Previous phases — corrected import paths\n",
    "from src.ingestion.cmapss_loader import CMAPSSDataLoader\n",
    "from src.features.pipeline import FeatureEngineeringPipeline\n",
    "from src.models.baseline_ml import XGBoostRULPredictor\n",
    "from src.anomaly import (\n",
    "    ResidualAnomalyDetector,\n",
    "    ChangePointDetector,\n",
    "    DegradationLabeler,\n",
    "    EarlyWarningSystem\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress chatty HTTP / tokenizer / RAG module logs\n",
    "for _lg in ['httpx', 'sentence_transformers', 'transformers',\n",
    "            'src.rag', 'src.rag.knowledge_base', 'src.rag.document_chunker',\n",
    "            'src.rag.embedder', 'src.rag.vector_store', 'src.rag.retriever']:\n",
    "    logging.getLogger(_lg).setLevel(logging.WARNING)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Constants\n",
    "SUBSETS = ['FD001', 'FD002', 'FD003', 'FD004']\n",
    "SENSOR_COLS = [f'sensor_{i}' for i in range(1, 22)]\n",
    "DATA_DIR = Path('../data/raw/CMAPSS')\n",
    "REPORTS_DIR = Path('../reports/figures')\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "KB_DIR = Path('../data/vector_db')\n",
    "KB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ All modules imported successfully (source reloaded)\")\n",
    "print(f\"✓ torch.get_num_threads() = {torch.get_num_threads()}\")\n",
    "print(f\"✓ Subsets: {SUBSETS}\")\n",
    "print(f\"✓ Notebook ready for PHASE 6: RAG Pipeline Integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6ee87",
   "metadata": {},
   "source": [
    "## Section 2: Generate Degradation Data (PHASE 5 Output)\n",
    "\n",
    "Run PHASE 5 pipeline to generate degradation periods for knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef200110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-27 02:33:26 - __main__ - INFO - Loading all 4 C-MAPSS subsets...\n",
      "2026-02-27 02:33:26 - src.ingestion.cmapss_loader - INFO - Loaded FD001: Train=20631 rows, Test=13096 rows\n",
      "2026-02-27 02:33:26 - src.ingestion.cmapss_loader - INFO - Loaded FD002: Train=53759 rows, Test=33991 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FD001: train 20,631 rows / 100 engines  |  test 13,096 rows / 100 engines\n",
      "  FD002: train 53,759 rows / 260 engines  |  test 33,991 rows / 259 engines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-27 02:33:26 - src.ingestion.cmapss_loader - INFO - Loaded FD003: Train=24720 rows, Test=16596 rows\n",
      "2026-02-27 02:33:26 - src.ingestion.cmapss_loader - INFO - Loaded FD004: Train=61249 rows, Test=41214 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FD003: train 24,720 rows / 100 engines  |  test 16,596 rows / 100 engines\n",
      "  FD004: train 61,249 rows / 249 engines  |  test 41,214 rows / 248 engines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-27 02:33:26 - __main__ - INFO - Engineering features...\n",
      "2026-02-27 02:33:26 - src.features.pipeline - INFO - Fitting feature engineering pipeline...\n",
      "2026-02-27 02:33:26 - src.features.pipeline - INFO - Step 1: Generating sliding windows\n",
      "2026-02-27 02:33:26 - src.features.sliding_windows - INFO - Generating sliding windows (size=30, step=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Combined train: 160,359 rows, 709 engines\n",
      "✓ Combined test:  104,897 rows, 707 engines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-27 02:33:30 - src.features.sliding_windows - INFO - Generated 157523 windows from 709 engines\n",
      "2026-02-27 02:33:30 - src.features.sliding_windows - INFO - Window shape: (157523, 30, 24) (num_windows, window_size, features)\n",
      "2026-02-27 02:33:30 - src.features.pipeline - INFO - Step 2: Calculating health indicators\n",
      "2026-02-27 02:33:32 - src.features.health_indicators - INFO - Calculated combined health index using mean method\n",
      "2026-02-27 02:33:32 - src.features.pipeline - INFO - Step 3: Engineering time-series features\n",
      "2026-02-27 02:33:39 - src.features.engineering - INFO - Added rolling statistics for windows: [5, 10, 20]\n",
      "2026-02-27 02:33:41 - src.features.engineering - INFO - Added EWMA features for spans: [5, 10, 20]\n",
      "2026-02-27 02:33:41 - src.features.engineering - INFO - Added difference features for lags: [1, 5, 10]\n",
      "2026-02-27 02:33:42 - src.features.engineering - INFO - Added 5 Fourier feature pairs\n",
      "2026-02-27 02:33:42 - src.features.pipeline - INFO - Engineered features: 431 features\n",
      "2026-02-27 02:33:42 - src.features.pipeline - INFO - Step 4: Selecting features using combined method\n",
      "2026-02-27 02:33:43 - src.features.feature_selection - INFO - Variance selection: 431 -> 402 features\n",
      "2026-02-27 02:33:43 - src.features.feature_selection - INFO - Correlation selection: 402 -> 30 features\n",
      "2026-02-27 02:33:43 - src.features.feature_selection - INFO - Top 5 features by f_regression score:\n",
      "fourier_cos_1      90479.330468\n",
      "fourier_sin_2      34785.713573\n",
      "fourier_cos_2      18795.015260\n",
      "fourier_sin_3      15220.954274\n",
      "sensor_11_drift    12931.863020\n",
      "dtype: float64\n",
      "2026-02-27 02:33:43 - src.features.feature_selection - INFO - Tree importance: sub-sampled 160359 -> 48107 rows (sample_frac=0.3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mEngineering features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m pipeline = FeatureEngineeringPipeline(window_size=\u001b[32m30\u001b[39m, window_step=\u001b[32m1\u001b[39m, scale_features=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m X_train, y_train = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSENSOR_COLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRUL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_selection_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcombined\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m X_test, y_test = pipeline.transform(df_test)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Features — train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/src/features/pipeline.py:271\u001b[39m, in \u001b[36mFeatureEngineeringPipeline.fit_transform\u001b[39m\u001b[34m(self, df, sensor_cols, target_col, feature_selection_method, selection_k)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\n\u001b[32m    241\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    242\u001b[39m     df: pd.DataFrame,\n\u001b[32m   (...)\u001b[39m\u001b[32m    246\u001b[39m     selection_k: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m,\n\u001b[32m    247\u001b[39m ) -> Tuple[np.ndarray, np.ndarray]:\n\u001b[32m    248\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m    Fit and transform in one step.\u001b[39;00m\n\u001b[32m    250\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    269\u001b[39m \u001b[33;03m        RUL labels\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_selection_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/src/features/pipeline.py:165\u001b[39m, in \u001b[36mFeatureEngineeringPipeline.fit\u001b[39m\u001b[34m(self, df, sensor_cols, target_col, feature_selection_method, selection_k)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_selection_method:\n\u001b[32m    164\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep 4: Selecting features using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_selection_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m method\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     X_selected, \u001b[38;5;28mself\u001b[39m.selected_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_engineered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengineered_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_selection_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselection_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     X_final = X_selected.values\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/src/features/pipeline.py:408\u001b[39m, in \u001b[36mFeatureEngineeringPipeline._select_features\u001b[39m\u001b[34m(self, X, y, method, k)\u001b[39m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(X_sel, columns=cols), cols\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_combined\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown selection method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/src/features/feature_selection.py:248\u001b[39m, in \u001b[36mFeatureSelector.select_combined\u001b[39m\u001b[34m(self, X, target, variance_threshold, correlation_k, tree_k)\u001b[39m\n\u001b[32m    245\u001b[39m _, feat_corr = \u001b[38;5;28mself\u001b[39m.select_by_correlation(X_var, target, k=correlation_k)\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Step 3: Tree importance filtering\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m _, feat_tree, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselect_by_tree_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtree_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# Step 4: Intersection (most robust features)\u001b[39;00m\n\u001b[32m    251\u001b[39m combined_features = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(feat_corr) & \u001b[38;5;28mset\u001b[39m(feat_tree))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/src/features/feature_selection.py:145\u001b[39m, in \u001b[36mFeatureSelector.select_by_tree_importance\u001b[39m\u001b[34m(self, X, target, k, sample_frac)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Reduced RF parameters to avoid multi-hour runtimes on CPU\u001b[39;00m\n\u001b[32m    139\u001b[39m rf = RandomForestRegressor(\n\u001b[32m    140\u001b[39m     n_estimators=\u001b[32m20\u001b[39m,   \u001b[38;5;66;03m# was 100\u001b[39;00m\n\u001b[32m    141\u001b[39m     max_depth=\u001b[32m8\u001b[39m,       \u001b[38;5;66;03m# was 10 / None\u001b[39;00m\n\u001b[32m    142\u001b[39m     n_jobs=\u001b[32m1\u001b[39m,          \u001b[38;5;66;03m# single-threaded to avoid joblib deadlock in Jupyter\u001b[39;00m\n\u001b[32m    143\u001b[39m     random_state=\u001b[38;5;28mself\u001b[39m.random_state,\n\u001b[32m    144\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[43mrf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Get importances\u001b[39;00m\n\u001b[32m    148\u001b[39m importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/.venv/lib/python3.14/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/.venv/lib/python3.14/site-packages/sklearn/ensemble/_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/.venv/lib/python3.14/site-packages/sklearn/utils/parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/.venv/lib/python3.14/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/.venv/lib/python3.14/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/.venv/lib/python3.14/site-packages/sklearn/utils/parallel.py:184\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m                 this_warning_filter_dict[special_key] = this_value.pattern\n\u001b[32m    182\u001b[39m         warnings.filterwarnings(**this_warning_filter_dict, append=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/.venv/lib/python3.14/site-packages/sklearn/ensemble/_forest.py:188\u001b[39m, in \u001b[36m_parallel_build_trees\u001b[39m\u001b[34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    186\u001b[39m         curr_sample_weight *= compute_sample_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, y, indices=indices)\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    196\u001b[39m     tree._fit(\n\u001b[32m    197\u001b[39m         X,\n\u001b[32m    198\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    201\u001b[39m         missing_values_in_feature_mask=missing_values_in_feature_mask,\n\u001b[32m    202\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GITHUB CAPSTONE /Agentic-Early-Warning-Intelligence-System-for-Silent-System-Failures/.venv/lib/python3.14/site-packages/sklearn/tree/_classes.py:475\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    465\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    466\u001b[39m         splitter,\n\u001b[32m    467\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    472\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    473\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    478\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ── 2A. Load ALL 4 C-MAPSS subsets ──────────────────────────────────\n",
    "logger.info(\"Loading all 4 C-MAPSS subsets...\")\n",
    "loader = CMAPSSDataLoader(data_dir=str(DATA_DIR))\n",
    "\n",
    "train_frames, test_frames = [], []\n",
    "for subset in SUBSETS:\n",
    "    tr, te, rul_te = loader.load_dataset(subset)\n",
    "    # Composite engine IDs to prevent cross-subset contamination\n",
    "    tr = tr.copy()\n",
    "    te = te.copy()\n",
    "    tr['engine_id'] = subset + '_' + tr['engine_id'].astype(int).astype(str)\n",
    "    te['engine_id'] = subset + '_' + te['engine_id'].astype(int).astype(str)\n",
    "    # Assign RUL to test rows (last-cycle RUL broadcast)\n",
    "    max_cycles = te.groupby('engine_id')['cycle'].transform('max')\n",
    "    te['RUL'] = (max_cycles - te['cycle']).values + rul_te.values[\n",
    "        te.groupby('engine_id').ngroup().values\n",
    "    ]\n",
    "    train_frames.append(tr)\n",
    "    test_frames.append(te)\n",
    "    print(f\"  {subset}: train {len(tr):,} rows / {tr['engine_id'].nunique()} engines  |  \"\n",
    "          f\"test {len(te):,} rows / {te['engine_id'].nunique()} engines\")\n",
    "\n",
    "df_train = pd.concat(train_frames, ignore_index=True)\n",
    "df_test  = pd.concat(test_frames,  ignore_index=True)\n",
    "print(f\"\\n✓ Combined train: {len(df_train):,} rows, {df_train['engine_id'].nunique()} engines\")\n",
    "print(f\"✓ Combined test:  {len(df_test):,} rows, {df_test['engine_id'].nunique()} engines\")\n",
    "\n",
    "# ── 2B. Feature engineering ─────────────────────────────────────────\n",
    "logger.info(\"Engineering features...\")\n",
    "pipeline = FeatureEngineeringPipeline(window_size=30, window_step=1, scale_features=True)\n",
    "X_train, y_train = pipeline.fit_transform(\n",
    "    df_train, sensor_cols=SENSOR_COLS, target_col='RUL',\n",
    "    feature_selection_method='combined', selection_k=20\n",
    ")\n",
    "X_test, y_test = pipeline.transform(df_test)\n",
    "print(f\"✓ Features — train {X_train.shape}, test {X_test.shape}\")\n",
    "\n",
    "# ── 2C. Train XGBoost model ────────────────────────────────────────\n",
    "logger.info(\"Training XGBoost model...\")\n",
    "model = XGBoostRULPredictor()\n",
    "model.fit(X_train, y_train, X_test[:1000], y_test[:1000])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# ── 2D. Anomaly detection pipeline ─────────────────────────────────\n",
    "logger.info(\"Detecting anomalies...\")\n",
    "residual_detector = ResidualAnomalyDetector(method='zscore', threshold=3.0)\n",
    "residual_detector.fit(residuals[:1000])\n",
    "anomalies = residual_detector.detect(residuals)\n",
    "anomaly_scores = residual_detector.get_anomaly_scores(residuals)\n",
    "\n",
    "logger.info(\"Detecting change points...\")\n",
    "cp_detector = ChangePointDetector(method='cusum', threshold=3.0)\n",
    "cp_detector.fit(y_test[:100])\n",
    "change_points = cp_detector.detect(y_test)\n",
    "\n",
    "# ── 2E. Degradation labeling ───────────────────────────────────────\n",
    "logger.info(\"Labeling degradation periods...\")\n",
    "labeler = DegradationLabeler(rul_threshold=100)\n",
    "degradation_df = labeler.label_degradation(\n",
    "    rul_values=y_test,\n",
    "    anomaly_flags=anomalies,\n",
    "    anomaly_scores=anomaly_scores,\n",
    "    change_points=change_points\n",
    ")\n",
    "degradation_periods = labeler.get_degradation_periods(degradation_df)\n",
    "\n",
    "# ── 2F. Normalise period keys & map engine IDs ─────────────────────\n",
    "# get_degradation_periods() returns 'start_idx'/'end_idx';\n",
    "# build_from_degradation_data() expects 'start'/'end'.\n",
    "engine_ids = df_test['engine_id'].values\n",
    "for period in degradation_periods:\n",
    "    # Add canonical 'start'/'end' keys expected by KnowledgeBase\n",
    "    period['start'] = period['start_idx']\n",
    "    period['end']   = period['end_idx']\n",
    "    # Map composite engine ID from the test-set index\n",
    "    idx = period['start_idx']\n",
    "    if idx < len(engine_ids):\n",
    "        period['engine_id'] = str(engine_ids[idx])\n",
    "\n",
    "# Per-subset summary\n",
    "from collections import Counter\n",
    "subset_counts = Counter(\n",
    "    str(p.get('engine_id', '')).split('_')[0] for p in degradation_periods\n",
    ")\n",
    "print(f\"\\n✓ Generated {len(degradation_periods)} degradation periods\")\n",
    "print(f\"✓ Total test cycles: {len(y_test):,}\")\n",
    "print(f\"✓ Anomalies detected: {int(np.sum(anomalies)):,} ({np.mean(anomalies):.1%})\")\n",
    "print(f\"✓ Change points: {len(change_points)}\")\n",
    "print(f\"✓ Per-subset periods: {dict(subset_counts)}\")\n",
    "if degradation_periods:\n",
    "    print(f\"\\nExample period:\\n{degradation_periods[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f91b9",
   "metadata": {},
   "source": [
    "## Section 3: Build Knowledge Base\n",
    "\n",
    "Create knowledge base from degradation periods using RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5af00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/5: Loading embedding model …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1524.57it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  done [3.7s]\n",
      "Step 2/5: Creating documents …\n",
      "  303 documents [0.9s]\n",
      "Step 3/5: Chunking documents …\n",
      "  303 chunks [0.0s]\n",
      "Step 4/5: Embedding chunks …\n"
     ]
    }
   ],
   "source": [
    "import time, json, subprocess, tempfile\n",
    "\n",
    "# ── Silence verbose loggers that flood Jupyter output ────────────────\n",
    "for _name in ['src.rag', 'src.rag.knowledge_base', 'src.rag.document_chunker',\n",
    "              'src.rag.embedder', 'src.rag.vector_store', 'src.rag.retriever']:\n",
    "    logging.getLogger(_name).setLevel(logging.WARNING)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Subprocess-safe encoder\n",
    "#   model.encode() deadlocks inside Jupyter kernels due to\n",
    "#   tokenizers / OpenMP fork issues.  We shell out to a fresh\n",
    "#   Python process that encodes and saves a .npy file.\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "def _subprocess_encode(texts, model_name='all-MiniLM-L6-v2',\n",
    "                       batch_size=64, normalize=True):\n",
    "    \"\"\"Encode texts via subprocess to avoid Jupyter deadlock.\"\"\"\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    texts_path = os.path.join(tmp_dir, 'texts.json')\n",
    "    emb_path   = os.path.join(tmp_dir, 'embeddings.npy')\n",
    "\n",
    "    with open(texts_path, 'w') as f:\n",
    "        json.dump(texts if isinstance(texts, list) else [texts], f)\n",
    "\n",
    "    script = (\n",
    "        \"import os, json, numpy as np;\"\n",
    "        \"os.environ['TOKENIZERS_PARALLELISM']='false';\"\n",
    "        \"os.environ['OMP_NUM_THREADS']='1';\"\n",
    "        \"os.environ['MKL_NUM_THREADS']='1';\"\n",
    "        \"from sentence_transformers import SentenceTransformer;\"\n",
    "        f\"m=SentenceTransformer({model_name!r});\"\n",
    "        f\"ts=json.load(open({texts_path!r}));\"\n",
    "        f\"e=m.encode(ts,show_progress_bar=False,batch_size={batch_size},\"\n",
    "        f\"normalize_embeddings={normalize});\"\n",
    "        f\"np.save({emb_path!r},e)\"\n",
    "    )\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, '-c', script],\n",
    "        capture_output=True, text=True, timeout=300,\n",
    "        env={**os.environ, 'TOKENIZERS_PARALLELISM': 'false',\n",
    "             'OMP_NUM_THREADS': '1', 'MKL_NUM_THREADS': '1'}\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Encoding failed: {result.stderr[:500]}\")\n",
    "\n",
    "    embs = np.load(emb_path)\n",
    "    os.remove(texts_path); os.remove(emb_path); os.rmdir(tmp_dir)\n",
    "    return embs\n",
    "\n",
    "\n",
    "# ── Step 1: Initialize Knowledge Base ────────────────────────────────\n",
    "t0 = time.time()\n",
    "print(\"Step 1/5: Loading embedding model …\")\n",
    "kb = KnowledgeBase(\n",
    "    embedding_model='all-MiniLM-L6-v2',\n",
    "    chunk_size=500,\n",
    "    chunk_strategy='sentence'\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"  done [{t1 - t0:.1f}s]\")\n",
    "\n",
    "# ── Step 2: Create documents from degradation periods ────────────────\n",
    "print(\"Step 2/5: Creating documents …\")\n",
    "from src.rag.document_chunker import create_failure_document\n",
    "kb.documents = []\n",
    "for i, period in enumerate(degradation_periods):\n",
    "    engine_id = period.get('engine_id', 0)\n",
    "    sensor_stats = kb._extract_sensor_stats(df_test, engine_id,\n",
    "                                             period.get('start', 0),\n",
    "                                             period.get('end', 0))\n",
    "    doc = create_failure_document(engine_id=engine_id,\n",
    "                                  degradation_period=period,\n",
    "                                  sensor_data=sensor_stats)\n",
    "    kb.documents.append(doc)\n",
    "t2 = time.time()\n",
    "print(f\"  {len(kb.documents)} documents [{t2 - t1:.1f}s]\")\n",
    "\n",
    "# ── Step 3: Chunk documents ──────────────────────────────────────────\n",
    "print(\"Step 3/5: Chunking documents …\")\n",
    "kb.chunks = kb.chunker.chunk_documents(\n",
    "    kb.documents, text_field='text',\n",
    "    metadata_fields=['engine_id', 'failure_type', 'duration', 'severity']\n",
    ")\n",
    "t3 = time.time()\n",
    "print(f\"  {len(kb.chunks)} chunks [{t3 - t2:.1f}s]\")\n",
    "\n",
    "# ── Step 4: Generate embeddings (subprocess) ─────────────────────────\n",
    "print(\"Step 4/5: Embedding chunks (subprocess) …\")\n",
    "texts = [c['text'] for c in kb.chunks]\n",
    "kb.embeddings = _subprocess_encode(\n",
    "    texts, model_name='all-MiniLM-L6-v2', batch_size=64,\n",
    "    normalize=kb.embedder.normalize\n",
    ")\n",
    "t4 = time.time()\n",
    "print(f\"  shape {kb.embeddings.shape} [{t4 - t3:.1f}s]\")\n",
    "\n",
    "# ── Step 5: Build FAISS index & retriever, save ──────────────────────\n",
    "print(\"Step 5/5: Building vector store & saving …\")\n",
    "kb.vector_store = VectorStore(\n",
    "    embedding_dim=kb.embedder.embedding_dim,\n",
    "    index_type='Flat', metric='cosine'\n",
    ")\n",
    "kb.vector_store.add(embeddings=kb.embeddings, documents=kb.chunks)\n",
    "kb.retriever = Retriever(\n",
    "    vector_store=kb.vector_store, embedder=kb.embedder,\n",
    "    top_k=5, min_similarity=0.3, include_citations=True\n",
    ")\n",
    "kb.save(str(KB_DIR))\n",
    "t5 = time.time()\n",
    "print(f\"  saved to '{KB_DIR}' [{t5 - t4:.1f}s]\")\n",
    "\n",
    "# ── Monkey-patch embedder so downstream kb.search() won't deadlock ───\n",
    "_orig_embed_text = kb.embedder.embed_text\n",
    "\n",
    "def _safe_embed_text(text, show_progress=False, batch_size=32):\n",
    "    \"\"\"Subprocess-safe embed_text replacement for Jupyter.\"\"\"\n",
    "    is_single = isinstance(text, str)\n",
    "    texts_list = [text] if is_single else list(text)\n",
    "    embs = _subprocess_encode(\n",
    "        texts_list, model_name=kb.embedder.model_name,\n",
    "        batch_size=batch_size, normalize=kb.embedder.normalize\n",
    "    )\n",
    "    return embs[0] if is_single else embs\n",
    "\n",
    "kb.embedder.embed_text = _safe_embed_text\n",
    "print(\"  ✓ Embedder patched for subprocess-safe search\")\n",
    "\n",
    "# ── Statistics ───────────────────────────────────────────────────────\n",
    "stats = kb.get_statistics()\n",
    "print(f\"\\n✓ Knowledge Base Built Successfully!\")\n",
    "print(f\"  - Documents: {stats['n_documents']}\")\n",
    "print(f\"  - Chunks: {stats['n_chunks']}\")\n",
    "print(f\"  - Embedding model: {stats['embedding_model']}\")\n",
    "print(f\"  - Embedding dimension: {stats['embedding_dim']}\")\n",
    "if 'mean_chunk_size' in stats:\n",
    "    print(f\"  - Mean chunk size: {stats['mean_chunk_size']:.0f} characters\")\n",
    "if 'vector_store_size' in stats:\n",
    "    print(f\"  - Vector store size: {stats['vector_store_size']}\")\n",
    "\n",
    "# ── Chunk size distribution plot ─────────────────────────────────────\n",
    "chunk_sizes = [len(c.get('text', '')) for c in kb.chunks]\n",
    "if chunk_sizes:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].hist(chunk_sizes, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[0].set_xlabel('Chunk Size (characters)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Chunk Size Distribution')\n",
    "    axes[0].axvline(np.mean(chunk_sizes), color='red', linestyle='--',\n",
    "                    label=f'Mean: {np.mean(chunk_sizes):.0f}')\n",
    "    axes[0].legend()\n",
    "    axes[1].boxplot(chunk_sizes, vert=True)\n",
    "    axes[1].set_ylabel('Chunk Size (characters)')\n",
    "    axes[1].set_title('Chunk Size Box Plot')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(REPORTS_DIR / 'rag_chunk_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n✓ Total wall time: {time.time() - t0:.1f}s\")\n",
    "\n",
    "# Restore logging\n",
    "for _name in ['src.rag', 'src.rag.knowledge_base', 'src.rag.document_chunker',\n",
    "              'src.rag.embedder', 'src.rag.vector_store', 'src.rag.retriever']:\n",
    "    logging.getLogger(_name).setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf7735",
   "metadata": {},
   "source": [
    "## Section 4: Similarity Search & Retrieval\n",
    "\n",
    "Test retrieval with the target query: \"Find past incidents similar to current sensor deviation pattern\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84080d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Query 1: text query ──────────────────────────────────────────────\n",
    "query1 = \"Find past incidents similar to current sensor deviation pattern with high temperature\"\n",
    "print(f\"Query 1: {query1}\\n\")\n",
    "\n",
    "results1 = kb.search(query1, top_k=5)\n",
    "print(f\"Retrieved {len(results1)} results:\\n\")\n",
    "for i, result in enumerate(results1, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"  Score: {result['score']:.3f}\")\n",
    "    print(f\"  Text:  {result['text'][:200]}...\")\n",
    "    citation = result.get('citation', {})\n",
    "    if citation:\n",
    "        print(f\"  Citation: {citation.get('citation_string', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "# ── Query 2: degradation pattern ────────────────────────────────────\n",
    "print(\"=\" * 80)\n",
    "query2 = \"Silent degradation with gradual RUL decrease and multiple anomalies detected\"\n",
    "print(f\"\\nQuery 2: {query2}\\n\")\n",
    "\n",
    "results2 = kb.search(query2, top_k=5)\n",
    "print(f\"Retrieved {len(results2)} results:\\n\")\n",
    "for i, result in enumerate(results2, 1):\n",
    "    meta = result.get('metadata', {})\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"  Score: {result['score']:.3f}\")\n",
    "    print(f\"  Text:  {result['text'][:150]}...\")\n",
    "    if 'engine_id' in meta:\n",
    "        print(f\"  Engine: {meta['engine_id']}\")\n",
    "    if 'duration' in meta:\n",
    "        print(f\"  Duration: {meta['duration']} cycles\")\n",
    "    print()\n",
    "\n",
    "# ── Query 3: sensor deviation pattern ───────────────────────────────\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nQuery 3: Current sensor deviation pattern\\n\")\n",
    "\n",
    "sensor_deviations = {\n",
    "    'sensor_2':  0.45,   # Temperature increase\n",
    "    'sensor_3': -0.32,   # Pressure decrease\n",
    "    'sensor_4':  0.28,   # Speed deviation\n",
    "    'sensor_11': 0.51,   # High deviation\n",
    "    'sensor_15': -0.19   # Minor decrease\n",
    "}\n",
    "\n",
    "results3 = kb.search_similar_failures(\n",
    "    sensor_deviations=sensor_deviations,\n",
    "    rul=75.0,\n",
    "    anomaly_score=0.65,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Sensor Deviations: {sensor_deviations}\")\n",
    "print(f\"RUL: 75.0, Anomaly Score: 0.65\\n\")\n",
    "print(f\"Retrieved {len(results3)} similar failures:\\n\")\n",
    "for i, result in enumerate(results3, 1):\n",
    "    citation = result.get('citation', {})\n",
    "    print(f\"Similar Failure {i}:\")\n",
    "    print(f\"  Similarity: {result['score']:.3f}\")\n",
    "    print(f\"  Description: {result['text'][:180]}...\")\n",
    "    if citation:\n",
    "        print(f\"  Citation: {citation.get('citation_string', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "# ── Visualise similarity scores ─────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for idx, (results, qname) in enumerate([\n",
    "    (results1, 'Query 1'), (results2, 'Query 2'), (results3, 'Query 3')\n",
    "]):\n",
    "    scores = [r['score'] for r in results]\n",
    "    ranks  = list(range(1, len(scores) + 1))\n",
    "    axes[idx].bar(ranks, scores, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel('Result Rank')\n",
    "    axes[idx].set_ylabel('Similarity Score')\n",
    "    axes[idx].set_title(f'{qname}\\nSimilarity Scores')\n",
    "    axes[idx].set_xticks(ranks)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(REPORTS_DIR / 'rag_similarity_scores.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Similarity search completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b630aa4",
   "metadata": {},
   "source": [
    "## Section 5: Validate Retrieval Relevance\n",
    "\n",
    "Manual validation of retrieval quality using test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aaee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_test_cases_from_degradation already imported in Cell 3\n",
    "\n",
    "logger.info(\"Creating test cases for validation...\")\n",
    "test_cases = create_test_cases_from_degradation(\n",
    "    degradation_periods,\n",
    "    n_test_cases=20\n",
    ")\n",
    "\n",
    "print(f\"Created {len(test_cases)} test cases\\n\")\n",
    "print(\"Example test cases:\")\n",
    "for i, tc in enumerate(test_cases[:3], 1):\n",
    "    print(f\"\\n  Test Case {i}:\")\n",
    "    print(f\"    Query: {tc['query']}\")\n",
    "    print(f\"    Expected: Engine {tc['expected_results']}\")\n",
    "\n",
    "# ── Validate retrieval ──────────────────────────────────────────────\n",
    "logger.info(\"Validating retrieval quality...\")\n",
    "validation_metrics = kb.validate(\n",
    "    test_cases=test_cases,\n",
    "    query_field='query',\n",
    "    expected_field='expected_results'\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"RETRIEVAL VALIDATION METRICS\")\n",
    "print(f\"{'=' * 60}\\n\")\n",
    "print(f\"Test Cases:          {validation_metrics['n_test_cases']}\")\n",
    "print(f\"Top-1 Accuracy:      {validation_metrics['top_1_accuracy']:.1%}\")\n",
    "print(f\"Top-5 Recall:        {validation_metrics['top_5_recall']:.1%}\")\n",
    "print(f\"Mean Reciprocal Rank:{validation_metrics['mrr']:.3f}\")\n",
    "\n",
    "# ── Manual inspection ───────────────────────────────────────────────\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"MANUAL INSPECTION (First 3 Test Cases)\")\n",
    "print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "for i, tc in enumerate(test_cases[:3], 1):\n",
    "    query = tc['query']\n",
    "    expected_engine = tc['expected_results']\n",
    "    print(f\"Test Case {i}:\")\n",
    "    print(f\"  Query: {query}\")\n",
    "    print(f\"  Expected Engine: {expected_engine}\\n  Top 5 Results:\")\n",
    "    results = kb.search(query, top_k=5)\n",
    "    for rank, r in enumerate(results, 1):\n",
    "        eid   = r.get('metadata', {}).get('engine_id', 'N/A')\n",
    "        score = r['score']\n",
    "        match = \"✓\" if eid == expected_engine else \" \"\n",
    "        print(f\"    {rank}. Engine {eid} | Score: {score:.3f} {match}\")\n",
    "    print()\n",
    "\n",
    "# ── Visualise validation ────────────────────────────────────────────\n",
    "metrics_names  = ['Top-1 Accuracy', 'Top-5 Recall', 'MRR']\n",
    "metrics_values = [\n",
    "    validation_metrics['top_1_accuracy'],\n",
    "    validation_metrics['top_5_recall'],\n",
    "    validation_metrics['mrr']\n",
    "]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['#2ecc71' if v >= 0.7 else '#e74c3c' for v in metrics_values]\n",
    "bars = ax1.bar(metrics_names, metrics_values, color=colors, edgecolor='black', alpha=0.7)\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Retrieval Validation Metrics')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.axhline(0.7, color='orange', linestyle='--', label='Target (70%)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2., bar.get_height(),\n",
    "             f'{value:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Retrieval quality heatmap\n",
    "n_test = len(test_cases)\n",
    "quality_matrix = np.zeros((n_test, 5))\n",
    "for i, tc in enumerate(test_cases):\n",
    "    results = kb.search(tc['query'], top_k=5)\n",
    "    expected = tc['expected_results']\n",
    "    for rank, r in enumerate(results):\n",
    "        eid = r.get('metadata', {}).get('engine_id', -1)\n",
    "        if eid == expected:\n",
    "            quality_matrix[i, rank] = r['score']\n",
    "\n",
    "n_show = min(15, n_test)\n",
    "im = ax2.imshow(quality_matrix[:n_show], aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "ax2.set_xlabel('Result Rank')\n",
    "ax2.set_ylabel('Test Case')\n",
    "ax2.set_title('Retrieval Quality Heatmap\\n(Similarity when expected result found)')\n",
    "ax2.set_xticks(range(5))\n",
    "ax2.set_xticklabels([f'Rank {i+1}' for i in range(5)])\n",
    "ax2.set_yticks(range(n_show))\n",
    "ax2.set_yticklabels([f'TC {i+1}' for i in range(n_show)])\n",
    "plt.colorbar(im, ax=ax2, label='Similarity Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(REPORTS_DIR / 'rag_validation_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Pass / Fail\n",
    "target_top5 = 0.70\n",
    "if validation_metrics['top_5_recall'] >= target_top5:\n",
    "    print(f\"\\n✓ VALIDATION PASSED: Top-5 recall {validation_metrics['top_5_recall']:.1%} >= {target_top5:.0%}\")\n",
    "else:\n",
    "    print(f\"\\n✗ VALIDATION BELOW TARGET: Top-5 recall {validation_metrics['top_5_recall']:.1%} < {target_top5:.0%}\")\n",
    "print(\"✓ Validation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc274b",
   "metadata": {},
   "source": [
    "## Section 6: Citation Tracking & Export\n",
    "\n",
    "Demonstrate citation tracking and export results for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json\n",
    "\n",
    "# ── Retrieve with full citations ────────────────────────────────────\n",
    "query = \"Find past incidents similar to current sensor deviation pattern\"\n",
    "results = kb.search(query, top_k=5)\n",
    "\n",
    "print(\"CITATION TRACKING DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nQuery: {query}\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Text: {result['text'][:250]}...\")\n",
    "    meta = result.get('metadata', {})\n",
    "    if meta:\n",
    "        print(f\"\\nMetadata:\")\n",
    "        for k, v in meta.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    citation = result.get('citation', {})\n",
    "    if citation:\n",
    "        print(f\"\\nCitation Information:\")\n",
    "        for k, v in citation.items():\n",
    "            if k != 'citation_string':\n",
    "                print(f\"  {k}: {v}\")\n",
    "        print(f\"\\nFormatted Citation:\\n  {citation.get('citation_string', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "# ── Export results ──────────────────────────────────────────────────\n",
    "reports_dir = Path('../reports')\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_export = []\n",
    "for result in results:\n",
    "    citation = result.get('citation', {})\n",
    "    meta = result.get('metadata', {})\n",
    "    results_export.append({\n",
    "        'rank':         result.get('rank', 0),\n",
    "        'score':        result['score'],\n",
    "        'text':         result['text'],\n",
    "        'engine_id':    meta.get('engine_id', None),\n",
    "        'failure_type': meta.get('failure_type', None),\n",
    "        'duration':     meta.get('duration', None),\n",
    "        'citation':     citation.get('citation_string', ''),\n",
    "        'retrieved_at': citation.get('retrieved_at', '')\n",
    "    })\n",
    "\n",
    "csv_path = reports_dir / 'rag_retrieval_results.csv'\n",
    "if results_export:\n",
    "    with open(csv_path, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=results_export[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results_export)\n",
    "    print(f\"✓ Results exported to {csv_path}\")\n",
    "\n",
    "json_path = reports_dir / 'rag_retrieval_results.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results_export, f, indent=2, default=str)\n",
    "print(f\"✓ Results exported to {json_path}\")\n",
    "\n",
    "# ── Query history analytics ─────────────────────────────────────────\n",
    "query_history = kb.retriever.get_query_history()\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"QUERY HISTORY ANALYTICS\")\n",
    "print(f\"{'=' * 80}\\n\")\n",
    "print(f\"Total queries: {len(query_history)}\")\n",
    "\n",
    "if query_history:\n",
    "    print(f\"\\nRecent queries:\")\n",
    "    for i, qr in enumerate(query_history[-5:], 1):\n",
    "        print(f\"\\n  {i}. Query: {qr['query'][:80]}...\")\n",
    "        print(f\"     Results: {qr['n_results']}  |  Top score: {qr['top_score']:.3f}\")\n",
    "        print(f\"     Timestamp: {qr['timestamp']}\")\n",
    "\n",
    "# ── Retriever statistics ────────────────────────────────────────────\n",
    "retriever_stats = kb.retriever.get_statistics()\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"RETRIEVER STATISTICS\")\n",
    "print(f\"{'=' * 80}\\n\")\n",
    "for k, v in retriever_stats.items():\n",
    "    print(f\"  {k}: {v:.3f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\n✓ Citation tracking and export completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58bc13",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**PHASE 6 RAG Pipeline Complete — All 4 C-MAPSS Subsets (FD001-FD004)**\n",
    "\n",
    "**Data Coverage:**\n",
    "- ✓ FD001 (100 engines) + FD002 (260 engines) + FD003 (100 engines) + FD004 (248 engines)\n",
    "- ✓ Composite engine IDs (`FD001_10`, `FD002_3`, …) prevent cross-subset contamination\n",
    "- ✓ Feature engineering via `FeatureEngineeringPipeline` (30-cycle windows, 20 selected features)\n",
    "\n",
    "**Achievements:**\n",
    "- ✓ Built knowledge base from combined degradation data (all 4 subsets)\n",
    "- ✓ Document chunking with sentence-based strategy\n",
    "- ✓ Generated embeddings using sentence-transformers (`all-MiniLM-L6-v2`, 384-dim)\n",
    "- ✓ Created FAISS vector store for efficient similarity search\n",
    "- ✓ Implemented retrieval with citation tracking\n",
    "- ✓ Validated retrieval relevance (Top-5 recall target: ≥ 70 %)\n",
    "- ✓ Exported results to CSV/JSON under `reports/`\n",
    "\n",
    "**Key Metrics:**\n",
    "- Documents & chunks covering all subsets\n",
    "- Embedding dimension: 384\n",
    "- Retrieval validation: Top-1 accuracy, Top-5 recall, MRR\n",
    "- Citation tracking for every retrieved result\n",
    "\n",
    "**Next Steps:**\n",
    "- NB06: Agentic Architecture — integrate RAG retriever with reasoning agents\n",
    "- Optional: cross-encoder re-ranking, LLM-based natural language generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
