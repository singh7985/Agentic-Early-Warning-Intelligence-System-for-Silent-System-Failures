{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26dfed69",
   "metadata": {},
   "source": [
    "# PHASE 2: Data Ingestion & Understanding\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers:\n",
    "1. **Time-Series Data**: NASA C-MAPSS turbofan engine degradation dataset\n",
    "2. **Text Data**: LogHub system logs (HDFS, BGL)\n",
    "\n",
    "We will:\n",
    "- Download and parse the C-MAPSS dataset\n",
    "- Visualize sensor degradation patterns\n",
    "- Create RUL (Remaining Useful Life) labels\n",
    "- Split data by engine (train/val/test)\n",
    "- Download and normalize LogHub datasets\n",
    "- Create incident narratives from logs\n",
    "- Generate synthetic maintenance reports\n",
    "\n",
    "**Timeline**: Days 4‚Äì7 of project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc0dba",
   "metadata": {},
   "source": [
    "# Section 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb72ee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for datasets...\n",
      "‚úÖ Found C-MAPSS data in: /Users/xe/Desktop/CAPSTONE KRISHNA SIR /CMaps\n",
      "‚úÖ Project Root resolved to: /Users/xe\n",
      "‚ö†Ô∏è No specific 'maintenance/log' text files found. (Will check data/raw/sample_logs.txt)\n",
      "\n",
      "üìã Schema Defined: 26 columns\n",
      "   Metadata: ['engine_id', 'cycle']\n",
      "   Sensors:  ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21'] (Count: 21)\n",
      "üìÇ EDA reports  ‚Üí /Users/xe/reports/figures/eda\n",
      "üìÇ Interim data ‚Üí /Users/xe/data/interim\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure Visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# ==================== 1. AUTOMATIC DATASET DETECTION ====================\n",
    "print(\"üîç Scanning for datasets...\")\n",
    "\n",
    "# Define potential search paths (Project structure + User's Local Path)\n",
    "SEARCH_PATHS = [\n",
    "    Path(\"data/raw/CMAPSS\"),                            # Standard project path\n",
    "    Path(\"/Users/xe/Desktop/CAPSTONE KRISHNA SIR /CMaps/\"), # User's local path\n",
    "    Path(\"data/raw\"),                                   # Fallback\n",
    "    Path(\".\")                                           # Current dir\n",
    "]\n",
    "\n",
    "DATA_DIR = None\n",
    "for p in SEARCH_PATHS:\n",
    "    if p.exists() and list(p.glob(f\"train_FD001.txt\")):\n",
    "        DATA_DIR = p\n",
    "        print(f\"‚úÖ Found C-MAPSS data in: {DATA_DIR}\")\n",
    "        break\n",
    "\n",
    "if DATA_DIR is None:\n",
    "    raise FileNotFoundError(\"‚ùå Could not locate C-MAPSS files (train_FD001.txt, etc). Please check uploads.\")\n",
    "\n",
    "# Derive absolute project root from DATA_DIR: .../data/raw/CMAPSS ‚Üí project root\n",
    "PROJECT_ROOT     = DATA_DIR.resolve().parent.parent.parent\n",
    "INTERIM_DATA_DIR = PROJECT_ROOT / 'data' / 'interim'\n",
    "ED_REPORT_DIR    = PROJECT_ROOT / 'reports' / 'figures' / 'eda'\n",
    "INTERIM_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ED_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úÖ Project Root resolved to: {PROJECT_ROOT}\")\n",
    "\n",
    "# Scan for Text/Log Data\n",
    "TEXT_DATA_FILES = []\n",
    "for p in SEARCH_PATHS:\n",
    "    if p.exists():\n",
    "        logs = list(p.glob(\"*log*.txt\")) + list(p.glob(\"*maintenance*.txt\")) + list(p.glob(\"*doc*.txt\"))\n",
    "        if logs:\n",
    "            TEXT_DATA_FILES.extend(logs)\n",
    "\n",
    "if TEXT_DATA_FILES:\n",
    "    print(f\"‚úÖ Found extra text datasets: {[f.name for f in TEXT_DATA_FILES]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No specific 'maintenance/log' text files found. (Will check data/raw/sample_logs.txt)\")\n",
    "\n",
    "# ==================== 2. CONFIGURATION ====================\n",
    "# Columns based on C-MAPSS documentation (26 columns)\n",
    "# 1-2: Metadata (Engine, Cycle)\n",
    "# 3-5: Operational Settings\n",
    "# 6-26: Sensor Readings (s1 - s21)\n",
    "COL_NAMES = ['engine_id', 'cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + \\\n",
    "            [f'sensor_{i}' for i in range(1, 22)]\n",
    "\n",
    "print(f\"\\nüìã Schema Defined: {len(COL_NAMES)} columns\")\n",
    "print(f\"   Metadata: {COL_NAMES[:2]}\")\n",
    "print(f\"   Sensors:  {COL_NAMES[5:]} (Count: {len(COL_NAMES[5:])})\")\n",
    "print(f\"üìÇ EDA reports  ‚Üí {ED_REPORT_DIR}\")\n",
    "print(f\"üìÇ Interim data ‚Üí {INTERIM_DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cf6b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading datasets...\n",
      "   Reading train_FD001.txt...\n",
      "   Reading test_FD001.txt...\n",
      "   Reading RUL_FD001.txt...\n",
      "   Reading train_FD002.txt...\n",
      "   Reading test_FD002.txt...\n",
      "   Reading RUL_FD002.txt...\n",
      "   Reading train_FD003.txt...\n",
      "   Reading test_FD003.txt...\n",
      "   Reading RUL_FD003.txt...\n",
      "   Reading train_FD004.txt...\n",
      "   Reading test_FD004.txt...\n",
      "   Reading RUL_FD004.txt...\n",
      "\n",
      "‚úÖ Total Datasets Loaded: 12\n",
      "\n",
      "--- DATASET OVERVIEW ---\n",
      "Dataset: train_FD001\n",
      "   Shape: (20631, 27)\n",
      "   Engines: 100\n",
      "   Cycle Length (Avg/Min/Max): 206.3 / 128 / 362\n",
      "Dataset: test_FD001\n",
      "   Shape: (13096, 27)\n",
      "   Engines: 100\n",
      "   Cycle Length (Avg/Min/Max): 131.0 / 31 / 303\n",
      "Dataset: train_FD002\n",
      "   Shape: (53759, 27)\n",
      "   Engines: 260\n",
      "   Cycle Length (Avg/Min/Max): 206.8 / 128 / 378\n",
      "Dataset: test_FD002\n",
      "   Shape: (33991, 27)\n",
      "   Engines: 259\n",
      "   Cycle Length (Avg/Min/Max): 131.2 / 21 / 367\n",
      "Dataset: train_FD003\n",
      "   Shape: (24720, 27)\n",
      "   Engines: 100\n",
      "   Cycle Length (Avg/Min/Max): 247.2 / 145 / 525\n",
      "Dataset: test_FD003\n",
      "   Shape: (16596, 27)\n",
      "   Engines: 100\n",
      "   Cycle Length (Avg/Min/Max): 166.0 / 38 / 475\n",
      "Dataset: train_FD004\n",
      "   Shape: (61249, 27)\n",
      "   Engines: 249\n",
      "   Cycle Length (Avg/Min/Max): 246.0 / 128 / 543\n",
      "Dataset: test_FD004\n",
      "   Shape: (41214, 27)\n",
      "   Engines: 248\n",
      "   Cycle Length (Avg/Min/Max): 166.2 / 19 / 486\n",
      "\n",
      "‚úÖ Summary saved to: /Users/xe/reports/figures/eda/dataset_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# ==================== 3. DATASET LOADING & BASIC INFO ====================\n",
    "print(\"\\nLoading datasets...\")\n",
    "\n",
    "# Helper to load space-separated file (C-MAPSS format)\n",
    "def load_cmapss_file(fpath):\n",
    "    print(f\"   Reading {fpath.name}...\")\n",
    "    try:\n",
    "        # Load, use header=None, assign columns\n",
    "        df = pd.read_csv(fpath, sep=r\"\\s+\", header=None)\n",
    "        \n",
    "        # Infer headers if columns doesn't match\n",
    "        if df.shape[1] == 26:\n",
    "            df.columns = COL_NAMES\n",
    "        elif df.shape[1] == 28: # Some versions have trailing empty cols\n",
    "             df = df.iloc[:, :26]\n",
    "             df.columns = COL_NAMES\n",
    "        # Handle RUL files (Single column)\n",
    "        elif df.shape[1] == 1:\n",
    "            df.columns = ['RUL_end']\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {fpath.name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Dictionary to hold all DataFrames { 'train_FD001': df, 'test_FD001': df, ... }\n",
    "datasets = {}\n",
    "\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    train_f = DATA_DIR / f'train_{subset}.txt'\n",
    "    test_f = DATA_DIR / f'test_{subset}.txt'\n",
    "    rul_f = DATA_DIR / f'RUL_{subset}.txt'\n",
    "    \n",
    "    if train_f.exists():\n",
    "        d_train = load_cmapss_file(train_f)\n",
    "        d_train['subset'] = subset # Track source\n",
    "        datasets[f'train_{subset}'] = d_train\n",
    "        \n",
    "    if test_f.exists():\n",
    "        d_test = load_cmapss_file(test_f)\n",
    "        d_test['subset'] = subset\n",
    "        datasets[f'test_{subset}'] = d_test\n",
    "        \n",
    "    if rul_f.exists():\n",
    "        d_rul = load_cmapss_file(rul_f)\n",
    "        d_rul['subset'] = subset\n",
    "        # Add engine_id (1-indexed based on row number)\n",
    "        d_rul['engine_id'] = d_rul.index + 1\n",
    "        datasets[f'rul_{subset}'] = d_rul\n",
    "\n",
    "print(f\"\\n‚úÖ Total Datasets Loaded: {len(datasets)}\")\n",
    "\n",
    "# ==================== 4. BASIC STATISTICS ====================\n",
    "summary_rows = []\n",
    "\n",
    "print(\"\\n--- DATASET OVERVIEW ---\")\n",
    "for ds_name, df in datasets.items():\n",
    "    if 'rul' in ds_name: continue # Skip RUL files for generic stats\n",
    "    \n",
    "    # 1. Unique Engines\n",
    "    n_engines = df['engine_id'].nunique()\n",
    "    \n",
    "    # 2. Cycle Stats\n",
    "    cycle_stats = df.groupby('engine_id')['cycle'].agg(['min', 'max', 'count'])\n",
    "    avg_cycles = cycle_stats['max'].mean()\n",
    "    min_cycles = cycle_stats['max'].min()\n",
    "    max_cycles = cycle_stats['max'].max()\n",
    "    \n",
    "    # 3. Missing/Dupe\n",
    "    missing = df.isnull().sum().sum()\n",
    "    dupes = df.duplicated().sum()\n",
    "    \n",
    "    print(f\"Dataset: {ds_name}\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Engines: {n_engines}\")\n",
    "    print(f\"   Cycle Length (Avg/Min/Max): {avg_cycles:.1f} / {min_cycles} / {max_cycles}\")\n",
    "    \n",
    "    if missing > 0: print(f\"   ‚ö†Ô∏è WARNING: {missing} missing values found!\")\n",
    "    if dupes > 0:   print(f\"   ‚ö†Ô∏è WARNING: {dupes} duplicate rows found!\")\n",
    "    \n",
    "    summary_rows.append({\n",
    "        'Dataset': ds_name,\n",
    "        'Rows': len(df),\n",
    "        'Cols': df.shape[1],\n",
    "        'Engines': n_engines,\n",
    "        'Avg_Cycle': avg_cycles,\n",
    "        'Min_Cycle': min_cycles,\n",
    "        'Max_Cycle': max_cycles\n",
    "    })\n",
    "\n",
    "# Save Summary\n",
    "pd.DataFrame(summary_rows).to_csv(ED_REPORT_DIR / \"dataset_summary.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Summary saved to: {ED_REPORT_DIR}/dataset_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec18ca",
   "metadata": {},
   "source": [
    "# Section 2: Download and Parse C-MAPSS Time-Series Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962ae7b",
   "metadata": {},
   "source": [
    "## Step 1: Check Dataset Files\n",
    "\n",
    "First, let's check if the C-MAPSS dataset is already downloaded. If not, we can use the download script.\n",
    "\n",
    "**To download the dataset:**\n",
    "1. Set up Kaggle credentials: `~/.kaggle/kaggle.json`\n",
    "2. Run: `python scripts/download_cmapss.py`\n",
    "\n",
    "For now, we'll assume the files are available at `data/raw/CMAPSS/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42f77e",
   "metadata": {},
   "source": [
    "## Step 2: Load and Parse C-MAPSS Data\n",
    "\n",
    "We'll load all four C-MAPSS subsets (FD001‚ÄìFD004).\n",
    "\n",
    "Dataset structure:\n",
    "- **FD001**: 100 engines, 1 operating condition, 1 fault mode\n",
    "- **FD002**: 260 engines, 6 operating conditions, 1 fault mode\n",
    "- **FD003**: 100 engines, 1 operating condition, 2 fault modes\n",
    "- **FD004**: 248 engines, 6 operating conditions, 2 fault modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c3c92",
   "metadata": {},
   "source": [
    "## Step 3: Explore Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeef7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SENSOR ANALYSIS ---\n",
      "üìâ Analyzing Sensor Variance (Global Train Set)...\n",
      "‚úÖ No globally constant sensors found.\n",
      "\n",
      "--- Per-Subset Constant Sensors ---\n",
      "   FD001: ['sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "   FD002: None\n",
      "   FD003: ['sensor_1', 'sensor_5', 'sensor_16', 'sensor_18', 'sensor_19']\n",
      "   FD004: None\n",
      "‚úÖ Saved plot: /Users/xe/reports/figures/eda/cycle_distribution.png\n",
      "‚úÖ Saved plot: /Users/xe/reports/figures/eda/correlation_heatmap_FD001.png\n",
      "‚úÖ Saved plot: /Users/xe/reports/figures/eda/sensor_drift_check.png\n"
     ]
    }
   ],
   "source": [
    "# ==================== 5. SENSOR ANALYSIS & PLOTS ====================\n",
    "print(\"\\n--- SENSOR ANALYSIS ---\")\n",
    "\n",
    "# Combine all Train Data for global analysis (with subset ID)\n",
    "train_dfs = [datasets[k] for k in datasets if 'train' in k]\n",
    "full_train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "\n",
    "test_dfs = [datasets[k] for k in datasets if 'test' in k]\n",
    "full_test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "\n",
    "# 1. Identify Constant Sensors\n",
    "# Some sensors in C-MAPSS are constant (variance = 0), especially in FD001/003 (single condition)\n",
    "sensor_cols = [c for c in full_train_df.columns if 'sensor' in c]\n",
    "variances = full_train_df[sensor_cols].var()\n",
    "constant_sensors = variances[variances < 1e-5].index.tolist()\n",
    "\n",
    "print(f\"üìâ Analyzing Sensor Variance (Global Train Set)...\")\n",
    "if constant_sensors:\n",
    "    print(f\"‚ö†Ô∏è Found {len(constant_sensors)} Constant/Near-Constant Sensors (Global): {constant_sensors}\")\n",
    "else:\n",
    "    print(\"‚úÖ No globally constant sensors found.\")\n",
    "\n",
    "# Check Per-Subset Constancy (Important because FD001 is simpler than FD002)\n",
    "print(\"\\n--- Per-Subset Constant Sensors ---\")\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    sub_df = full_train_df[full_train_df['subset'] == subset]\n",
    "    sub_var = sub_df[sensor_cols].var()\n",
    "    const_sub = sub_var[sub_var < 1e-5].index.tolist()\n",
    "    if const_sub:\n",
    "        print(f\"   {subset}: {const_sub}\")\n",
    "    else:\n",
    "        print(f\"   {subset}: None\")\n",
    "\n",
    "# 2. Cycle Length Distribution Plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    # Get max cycle per engine for this subset\n",
    "    max_cycles = datasets[f'train_{subset}'].groupby('engine_id')['cycle'].max()\n",
    "    sns.kdeplot(max_cycles, label=subset, clip=(0, None))\n",
    "    \n",
    "plt.title(\"Engine Life Duration (Max Cycles) Distribution by Dataset\")\n",
    "plt.xlabel(\"Max Cycles\")\n",
    "plt.legend()\n",
    "plt.savefig(ED_REPORT_DIR / \"cycle_distribution.png\")\n",
    "plt.close()\n",
    "print(f\"‚úÖ Saved plot: {ED_REPORT_DIR}/cycle_distribution.png\")\n",
    "\n",
    "# 3. Sensor Correlation Heatmap (FD001 as baseline)\n",
    "# We use FD001 because generally it has the clearest degradation signal without multiple Op Conditions noise\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr = datasets['train_FD001'][sensor_cols].corr()\n",
    "sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1, annot=False)\n",
    "plt.title(\"Sensor Correlation Matrix (FD001)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ED_REPORT_DIR / \"correlation_heatmap_FD001.png\")\n",
    "plt.close()\n",
    "print(f\"‚úÖ Saved plot: {ED_REPORT_DIR}/correlation_heatmap_FD001.png\")\n",
    "\n",
    "# 4. Train vs Test Distribution (Drift Check)\n",
    "# Pick a highly variable sensor (e.g., Sensor 11 or 12)\n",
    "sensor_metric = 'sensor_11'\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(full_train_df[sensor_metric], label='Train (All)', shade=True, color='blue')\n",
    "sns.kdeplot(full_test_df[sensor_metric], label='Test (All)', shade=True, color='orange')\n",
    "plt.title(f\"{sensor_metric} Distribution: Train vs Test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Check drift per subset\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    sns.kdeplot(datasets[f'train_{subset}'][sensor_metric], label=f'{subset}', alpha=0.5)\n",
    "plt.title(f\"{sensor_metric} Distribution by Subset\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ED_REPORT_DIR / \"sensor_drift_check.png\")\n",
    "plt.close()\n",
    "print(f\"‚úÖ Saved plot: {ED_REPORT_DIR}/sensor_drift_check.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28483d6b",
   "metadata": {},
   "source": [
    "# Section 3: Merge, Save & Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e42ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEXT DATA EDA ---\n",
      "‚ö†Ô∏è No text log files processed.\n",
      "\n",
      "--- MERGING & SAVING ---\n",
      "‚úÖ Saved Combined Train: /Users/xe/data/interim/combined_train.csv (Shape: (160359, 28))\n",
      "‚úÖ Saved Combined Test:  /Users/xe/data/interim/combined_test.csv  (Shape: (104897, 28))\n",
      "‚úÖ Saved Combined RUL:   /Users/xe/data/interim/combined_test_rul.csv   (Shape: (707, 4))\n",
      "\n",
      "üéâ EDA PHASE COMPLETE. Proceed to Feature Engineering.\n"
     ]
    }
   ],
   "source": [
    "# ==================== 6. TEXT DATA EDA (LogHub / Maintenance Logs) ====================\n",
    "print(\"\\n--- TEXT DATA EDA ---\")\n",
    "\n",
    "combined_text_df = pd.DataFrame(columns=['source_file', 'line_content'])\n",
    "\n",
    "if TEXT_DATA_FILES:\n",
    "    for f in TEXT_DATA_FILES:\n",
    "        try:\n",
    "            # Read first few lines\n",
    "            with open(f, 'r', encoding='utf-8', errors='ignore') as log_file:\n",
    "                lines = log_file.readlines()\n",
    "                \n",
    "            print(f\"File: {f.name}\")\n",
    "            print(f\"   Shape: {len(lines)} lines\")\n",
    "            print(f\"   Preview (First 3 lines):\")\n",
    "            for i, line in enumerate(lines[:3]):\n",
    "                print(f\"     {i+1}: {line.strip()}\")\n",
    "            \n",
    "            # Simple DataFrame construction\n",
    "            temp_df = pd.DataFrame({'line_content': lines})\n",
    "            temp_df['source_file'] = f.name\n",
    "            combined_text_df = pd.concat([combined_text_df, temp_df], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading text file {f.name}: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No text log files processed.\")\n",
    "\n",
    "# Save Combined Text\n",
    "if not combined_text_df.empty:\n",
    "    combined_text_df.to_csv(INTERIM_DATA_DIR / \"combined_text_corpus.csv\", index=False)\n",
    "    print(f\"‚úÖ Combined text corpus saved: {INTERIM_DATA_DIR}/combined_text_corpus.csv\")\n",
    "\n",
    "# ==================== 7. MERGE & SAVE C-MAPSS DATASETS ====================\n",
    "print(\"\\n--- MERGING & SAVING ---\")\n",
    "\n",
    "# Ensure 'subset' col exists (added during loading)\n",
    "# Also add 'dataset_id' as requested (integer or string)\n",
    "# Let's use string 'FD001' etc as subset, and map to int dataset_id if needed.\n",
    "# User asked for 'dataset_id'. I'll map FD001->1, FD002->2 etc.\n",
    "\n",
    "dataset_map = {'FD001': 1, 'FD002': 2, 'FD003': 3, 'FD004': 4}\n",
    "\n",
    "# Process TRAIN\n",
    "full_train_df = pd.concat([datasets[k] for k in datasets if 'train' in k], ignore_index=True)\n",
    "full_train_df['dataset_id'] = full_train_df['subset'].map(dataset_map)\n",
    "full_train_df.to_csv(INTERIM_DATA_DIR / \"combined_train.csv\", index=False)\n",
    "print(f\"‚úÖ Saved Combined Train: {INTERIM_DATA_DIR}/combined_train.csv (Shape: {full_train_df.shape})\")\n",
    "\n",
    "# Process TEST\n",
    "full_test_df = pd.concat([datasets[k] for k in datasets if 'test' in k], ignore_index=True)\n",
    "full_test_df['dataset_id'] = full_test_df['subset'].map(dataset_map)\n",
    "full_test_df.to_csv(INTERIM_DATA_DIR / \"combined_test.csv\", index=False)\n",
    "print(f\"‚úÖ Saved Combined Test:  {INTERIM_DATA_DIR}/combined_test.csv  (Shape: {full_test_df.shape})\")\n",
    "\n",
    "# Process RUL (For Evaluation Later)\n",
    "full_rul_df = pd.concat([datasets[k] for k in datasets if 'rul' in k], ignore_index=True)\n",
    "full_rul_df['dataset_id'] = full_rul_df['subset'].map(dataset_map)\n",
    "full_rul_df.to_csv(INTERIM_DATA_DIR / \"combined_test_rul.csv\", index=False)\n",
    "print(f\"‚úÖ Saved Combined RUL:   {INTERIM_DATA_DIR}/combined_test_rul.csv   (Shape: {full_rul_df.shape})\")\n",
    "\n",
    "print(\"\\nüéâ EDA PHASE COMPLETE. Proceed to Feature Engineering.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdbc0891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ full_rul_df: (707, 4)  engines per subset: {1: 100, 2: 259, 3: 100, 4: 248}\n",
      "   engine_id  RUL_end subset  dataset_id\n",
      "0          1      112  FD001           1\n",
      "1          2       98  FD001           1\n",
      "2          3       69  FD001           1\n",
      "3          4       82  FD001           1\n",
      "4          5       91  FD001           1\n"
     ]
    }
   ],
   "source": [
    "# ==================== ASSEMBLE full_rul_df FROM ALL FOUR RUL FILES ====================\n",
    "# Combines rul_FD001 ‚Ä¶ rul_FD004 into a single DataFrame used by the RUL-label cell below.\n",
    "# dataset_map is defined in Cell 8: {'FD001': 1, 'FD002': 2, 'FD003': 3, 'FD004': 4}\n",
    "\n",
    "rul_pieces = []\n",
    "for subset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    key = f'rul_{subset}'\n",
    "    if key in datasets:\n",
    "        df = datasets[key][['engine_id', 'RUL_end', 'subset']].copy()\n",
    "        df['dataset_id'] = dataset_map[subset]   # FD001‚Üí1, FD002‚Üí2, etc.\n",
    "        rul_pieces.append(df)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Key '{key}' not found in datasets ‚Äî check that RUL_{subset}.txt loaded correctly.\")\n",
    "\n",
    "if not rul_pieces:\n",
    "    raise ValueError(\"‚ùå No RUL data assembled. Ensure all four RUL_FDxxx.txt files exist in DATA_DIR.\")\n",
    "\n",
    "full_rul_df = pd.concat(rul_pieces, ignore_index=True)\n",
    "print(f\"‚úÖ full_rul_df: {full_rul_df.shape}  engines per subset: \"\n",
    "      f\"{full_rul_df.groupby('dataset_id').size().to_dict()}\")\n",
    "print(full_rul_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a7b6a",
   "metadata": {},
   "source": [
    "# Section 4: Create Remaining Useful Life (RUL) Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24730af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RUL stats:\n",
      "                 RUL    RUL_clipped\n",
      "count  160359.000000  160359.000000\n",
      "mean      122.331338      90.182029\n",
      "std        83.538146      41.241036\n",
      "min         0.000000       0.000000\n",
      "25%        56.000000      56.000000\n",
      "50%       113.000000     113.000000\n",
      "75%       172.000000     125.000000\n",
      "max       542.000000     125.000000\n",
      "\n",
      "Test RUL stats:\n",
      "                 RUL    RUL_clipped\n",
      "count  104897.000000  104897.000000\n",
      "mean      162.628264     110.529195\n",
      "std        80.873737      26.863275\n",
      "min         6.000000       6.000000\n",
      "25%       107.000000     107.000000\n",
      "50%       154.000000     125.000000\n",
      "75%       206.000000     125.000000\n",
      "max       553.000000     125.000000\n",
      "\n",
      "Feature Engineering Step 1 (RUL Labels) Complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# RUL LABELS FOR ALL 4 C-MAPSS SUBSETS (FD001‚ÄìFD004)\n",
    "# =============================================================================\n",
    "# Composite key [dataset_id, engine_id] ensures FD001 engine 1 ‚â† FD002 engine 1.\n",
    "# Idempotent: safe to re-run without creating duplicate columns.\n",
    "\n",
    "# ---------- 1. TRAIN RUL (run-to-failure) ----------\n",
    "# Drop any leftover columns from a previous run\n",
    "for col in ['max_cycle', 'RUL', 'RUL_clipped']:\n",
    "    if col in full_train_df.columns:\n",
    "        full_train_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "max_cycle = (full_train_df\n",
    "             .groupby(['dataset_id', 'engine_id'])['cycle']\n",
    "             .transform('max'))\n",
    "full_train_df['max_cycle']   = max_cycle\n",
    "full_train_df['RUL']         = full_train_df['max_cycle'] - full_train_df['cycle']\n",
    "full_train_df['RUL_clipped'] = full_train_df['RUL'].clip(upper=125)\n",
    "\n",
    "# ---------- 2. TEST RUL (truncated ‚Äî uses official RUL_FDxxx.txt) ----------\n",
    "for col in ['max_cycle', 'RUL', 'RUL_clipped', 'RUL_end']:\n",
    "    if col in full_test_df.columns:\n",
    "        full_test_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "max_cycle_test = (full_test_df\n",
    "                  .groupby(['dataset_id', 'engine_id'])['cycle']\n",
    "                  .transform('max'))\n",
    "full_test_df['max_cycle'] = max_cycle_test\n",
    "\n",
    "# Merge with official RUL ‚Äî use dataset_id + engine_id, drop 'subset' from rul to\n",
    "# avoid duplicate subset_x / subset_y columns\n",
    "rul_merge = full_rul_df[['dataset_id', 'engine_id', 'RUL_end']].copy()\n",
    "full_test_df = full_test_df.merge(rul_merge, on=['dataset_id', 'engine_id'], how='left')\n",
    "\n",
    "full_test_df['RUL']         = full_test_df['RUL_end'] + (full_test_df['max_cycle'] - full_test_df['cycle'])\n",
    "full_test_df['RUL_clipped'] = full_test_df['RUL'].clip(upper=125)\n",
    "\n",
    "# ---------- 3. Summary ----------\n",
    "print(\"üìä RUL Labels ‚Äî ALL 4 C-MAPSS Subsets Combined\")\n",
    "print(f\"   Train: {full_train_df.shape}  subsets={sorted(full_train_df['subset'].unique())}\")\n",
    "print(f\"   Test:  {full_test_df.shape}   subsets={sorted(full_test_df['subset'].unique())}\")\n",
    "print(f\"\\nTrain RUL stats:\")\n",
    "print(full_train_df[['RUL', 'RUL_clipped']].describe().round(1))\n",
    "print(f\"\\nTest RUL stats:\")\n",
    "print(full_test_df[['RUL', 'RUL_clipped']].describe().round(1))\n",
    "\n",
    "# Per-subset engine counts\n",
    "print(\"\\nüìä Engines per subset:\")\n",
    "for split_name, df_ in [('Train', full_train_df), ('Test', full_test_df)]:\n",
    "    counts = df_.groupby('subset')['engine_id'].nunique().to_dict()\n",
    "    print(f\"   {split_name}: {counts}  total={sum(counts.values())}\")\n",
    "\n",
    "# ---------- 4. Save (ALL subsets combined) ----------\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "full_train_df.to_csv(PROCESSED_DIR / 'train_FD001.csv', index=False)\n",
    "full_test_df.to_csv(PROCESSED_DIR  / 'test_FD001.csv',  index=False)\n",
    "full_train_df.to_csv(INTERIM_DATA_DIR / 'train_with_rul.csv', index=False)\n",
    "full_test_df.to_csv(INTERIM_DATA_DIR  / 'test_with_rul.csv',  index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved to {PROCESSED_DIR}:\")\n",
    "print(f\"   train_FD001.csv  ({len(full_train_df):,} rows ‚Äî ALL subsets)\")\n",
    "print(f\"   test_FD001.csv   ({len(full_test_df):,} rows  ‚Äî ALL subsets)\")\n",
    "print(\"   ‚ö†Ô∏è  Filename says 'FD001' for backward compat, but contains FD001‚ÄìFD004 combined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26996c",
   "metadata": {},
   "source": [
    "# Section 5: Scaling, Validation Split & Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SCALING & SEQUENCING ---\n",
      "   dataset_id=1: global MinMaxScaler (19 features)\n",
      "   dataset_id=2: KMeans regime-based scaling (6 regimes, 24 features)\n",
      "   dataset_id=3: global MinMaxScaler (20 features)\n",
      "   dataset_id=4: KMeans regime-based scaling (6 regimes, 24 features)\n",
      "\n",
      "‚úÖ Shared feature columns for sequence generation: 19\n",
      "‚úÖ Scaler store saved to /Users/xe/models/feature_scaler.joblib\n",
      "\n",
      "Split Engines: 567 Train, 142 Val\n",
      "‚úÖ Saved: train=(129029, 31), val=(31330, 31), test=(104897, 33)\n",
      "\n",
      "Generating sequences (Window=30)...\n",
      "‚úÖ Train Sequences: (112586, 30, 19) | Labels: (112586,)\n",
      "‚úÖ Val Sequences:   (27212, 30, 19)   | Labels: (27212,)\n",
      "‚úÖ Test Sequences:  (84495, 30, 19)   | Labels: (84495,)\n",
      "‚úÖ Saved Numpy Sequences to /Users/xe/data/processed/sequences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==================== SCALING & SEQUENCE GENERATION (ALL 4 SUBSETS) ====================\n",
    "print(\"\\n--- SCALING & SEQUENCING (FD001 ‚Äì FD004 COMBINED) ---\")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import joblib\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def get_feature_cols(df):\n",
    "    \"\"\"\n",
    "    Returns columns suitable for scaling/modeling:\n",
    "    - Excludes metadata identifiers and target columns.\n",
    "    - Excludes constant columns (std == 0) to avoid NaN from MinMaxScaler.\n",
    "    \"\"\"\n",
    "    exclude = {\n",
    "        'dataset_id', 'engine_id', 'cycle', 'sub_dataset',\n",
    "        'subset', 'RUL', 'RUL_clipped', 'max_cycle', 'RUL_end'\n",
    "    }\n",
    "    return [c for c in df.columns if c not in exclude and df[c].std() > 0]\n",
    "\n",
    "# ---------- Regime-based scaler for multi-condition subsets ----------\n",
    "def fit_regime_scaler(train_df, sensor_cols, n_regimes=6):\n",
    "    \"\"\"\n",
    "    Cluster operating points via KMeans on op_settings, then fit one\n",
    "    MinMaxScaler per regime.  Returns (km_model, {regime_id: scaler}).\n",
    "    \"\"\"\n",
    "    km = KMeans(n_clusters=n_regimes, random_state=42, n_init=10)\n",
    "    km.fit(train_df[['op_setting_1', 'op_setting_2', 'op_setting_3']])\n",
    "    scalers = {}\n",
    "    for regime_id in range(n_regimes):\n",
    "        mask = km.labels_ == regime_id\n",
    "        sc = MinMaxScaler(feature_range=(-1, 1))\n",
    "        sc.fit(train_df.loc[mask, sensor_cols])\n",
    "        scalers[regime_id] = sc\n",
    "    return km, scalers\n",
    "\n",
    "def apply_regime_scaler(df, km, scalers, sensor_cols):\n",
    "    \"\"\"Predict regime for each row, then scale with the per-regime scaler.\"\"\"\n",
    "    df = df.copy()\n",
    "    labels = km.predict(df[['op_setting_1', 'op_setting_2', 'op_setting_3']])\n",
    "    for regime_id, sc in scalers.items():\n",
    "        mask = labels == regime_id\n",
    "        if mask.any():\n",
    "            df.loc[mask, sensor_cols] = sc.transform(df.loc[mask, sensor_cols])\n",
    "    return df\n",
    "\n",
    "# ---------- Scale each subset with its appropriate strategy ----------\n",
    "# FD001 (id=1) and FD003 (id=3): 1 operating condition ‚Üí global MinMaxScaler\n",
    "# FD002 (id=2) and FD004 (id=4): 6 operating conditions ‚Üí KMeans regime scaler\n",
    "\n",
    "MULTI_COND_SUBSETS = {2, 4}\n",
    "N_REGIMES = 6\n",
    "\n",
    "train_featured = full_train_df.copy()\n",
    "test_featured  = full_test_df.copy()\n",
    "\n",
    "scaler_store = {}          # persisted for inference\n",
    "train_scaled_pieces = []\n",
    "test_scaled_pieces  = []\n",
    "subset_feature_cols = {}   # track per-subset columns\n",
    "\n",
    "print(f\"üìä Scaling {sorted(train_featured['dataset_id'].unique())} subsets:\")\n",
    "\n",
    "for did in sorted(train_featured['dataset_id'].unique()):\n",
    "    tr = train_featured[train_featured['dataset_id'] == did].copy()\n",
    "    te = test_featured[test_featured['dataset_id'] == did].copy()\n",
    "    fcols = get_feature_cols(tr)\n",
    "    subset_feature_cols[did] = fcols\n",
    "\n",
    "    if did in MULTI_COND_SUBSETS:\n",
    "        # KMeans regime-based scaling ‚Äî preserves inter-regime sensor ranges\n",
    "        km, scalers = fit_regime_scaler(tr, fcols, n_regimes=N_REGIMES)\n",
    "        tr = apply_regime_scaler(tr, km, scalers, fcols)\n",
    "        te = apply_regime_scaler(te, km, scalers, fcols)\n",
    "        scaler_store[did] = {\n",
    "            'type': 'regime', 'km': km, 'scalers': scalers, 'feature_cols': fcols\n",
    "        }\n",
    "        print(f\"   dataset_id={did}: KMeans regime-based scaling \"\n",
    "              f\"({N_REGIMES} regimes, {len(fcols)} features, \"\n",
    "              f\"{len(tr)} train + {len(te)} test rows)\")\n",
    "    else:\n",
    "        # Single global scaler for single-condition subsets\n",
    "        sc = MinMaxScaler(feature_range=(-1, 1))\n",
    "        tr[fcols] = sc.fit_transform(tr[fcols])\n",
    "        te[fcols] = sc.transform(te[fcols])\n",
    "        scaler_store[did] = {'type': 'global', 'scaler': sc, 'feature_cols': fcols}\n",
    "        print(f\"   dataset_id={did}: global MinMaxScaler \"\n",
    "              f\"({len(fcols)} features, {len(tr)} train + {len(te)} test rows)\")\n",
    "\n",
    "    train_scaled_pieces.append(tr)\n",
    "    test_scaled_pieces.append(te)\n",
    "\n",
    "train_featured = pd.concat(train_scaled_pieces, ignore_index=True)\n",
    "test_featured  = pd.concat(test_scaled_pieces,  ignore_index=True)\n",
    "\n",
    "# feature_cols for sequence generation = intersection across all subsets\n",
    "# (guarantees every subset has a value for every feature ‚Äî no NaN-padded sequences)\n",
    "all_fcols_sets = [set(v) for v in subset_feature_cols.values()]\n",
    "feature_cols = sorted(set.intersection(*all_fcols_sets))\n",
    "print(f\"\\n‚úÖ Shared feature columns for sequence generation: {len(feature_cols)}\")\n",
    "\n",
    "# Save all scalers in one file\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(scaler_store, MODELS_DIR / 'feature_scaler.joblib')\n",
    "print(f\"‚úÖ Scaler store saved to {MODELS_DIR / 'feature_scaler.joblib'}\")\n",
    "\n",
    "# ---------- Validation Split ‚Äî GroupShuffleSplit keeps ALL cycles of each engine together ----------\n",
    "# Uses composite key (dataset_id + engine_id) so FD001 engine 1 ‚â† FD002 engine 1\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "grp = train_featured[['dataset_id', 'engine_id']].apply(\n",
    "    lambda r: f\"{r.dataset_id}_{r.engine_id}\", axis=1)\n",
    "tr_idx, va_idx = next(gss.split(train_featured, groups=grp))\n",
    "train_split = train_featured.iloc[tr_idx].copy()\n",
    "val_split   = train_featured.iloc[va_idx].copy()\n",
    "\n",
    "# Verify no engine leakage\n",
    "tr_keys = set(train_split['dataset_id'].astype(str) + '_' + train_split['engine_id'].astype(str))\n",
    "va_keys = set(val_split['dataset_id'].astype(str) + '_' + val_split['engine_id'].astype(str))\n",
    "assert len(tr_keys & va_keys) == 0, \"‚ùå ENGINE LEAKAGE between train/val!\"\n",
    "print(f\"\\n‚úÖ Train/Val Split (no engine leakage):\")\n",
    "print(f\"   Train: {train_split.shape}  engines={len(tr_keys)}\")\n",
    "print(f\"   Val:   {val_split.shape}    engines={len(va_keys)}\")\n",
    "print(f\"   Test:  {test_featured.shape}\")\n",
    "\n",
    "# Save processed CSVs ‚Äî these contain ALL 4 subsets combined\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "train_split.to_csv(PROCESSED_DIR / 'train_FD001.csv',  index=False)\n",
    "val_split.to_csv(  PROCESSED_DIR / 'val_FD001.csv',    index=False)\n",
    "test_featured.to_csv(PROCESSED_DIR / 'test_FD001.csv', index=False)\n",
    "\n",
    "# Verify all subsets present in output\n",
    "for name, df_ in [('train_FD001.csv', train_split), ('val_FD001.csv', val_split), ('test_FD001.csv', test_featured)]:\n",
    "    subs = sorted(df_['subset'].unique()) if 'subset' in df_.columns else ['unknown']\n",
    "    print(f\"   üíæ {name}: {len(df_):,} rows ‚Äî subsets: {subs}\")\n",
    "print(\"   ‚ÑπÔ∏è  Filenames say 'FD001' for backward compat, but contain ALL subsets (FD001‚ÄìFD004).\")\n",
    "\n",
    "# Alias for sequence generation below\n",
    "X_train_df = train_split\n",
    "X_val_df   = val_split\n",
    "\n",
    "# ---------- Sequence Generation Function (Sliding Window) ----------\n",
    "def create_sequences(df, feature_cols, sequence_length=30, pad=True):\n",
    "    \"\"\"\n",
    "    Creates (samples, seq_len, features) tensor from ALL subsets combined.\n",
    "    Uses composite key (dataset_id + engine_id) so windows don't cross engines.\n",
    "    If pad=True, engines shorter than seq_len are pre-padded with 0s.\n",
    "    Target is the RUL of the LAST step in the window.\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    for _, group in df.groupby(['dataset_id', 'engine_id']):\n",
    "        features = group[feature_cols].values\n",
    "        target   = group['RUL_clipped'].values\n",
    "        num_cycles = len(group)\n",
    "\n",
    "        if num_cycles >= sequence_length:\n",
    "            for i in range(num_cycles - sequence_length + 1):\n",
    "                X_seq.append(features[i : i + sequence_length])\n",
    "                y_seq.append(target[i + sequence_length - 1])\n",
    "        elif pad:\n",
    "            pad_len  = sequence_length - num_cycles\n",
    "            padding  = np.zeros((pad_len, features.shape[1]))\n",
    "            X_window = np.vstack([padding, features])\n",
    "            X_seq.append(X_window)\n",
    "            y_seq.append(target[-1])\n",
    "\n",
    "    return np.array(X_seq, dtype=np.float32), np.array(y_seq, dtype=np.float32)\n",
    "\n",
    "print(\"\\n‚è≥ Generating sequences (Window=30) for ALL subsets...\")\n",
    "SEQ_LEN = 30\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_df,   feature_cols, SEQ_LEN, pad=False)\n",
    "X_val_seq,   y_val_seq   = create_sequences(X_val_df,     feature_cols, SEQ_LEN, pad=False)\n",
    "X_test_seq,  y_test_seq  = create_sequences(test_featured, feature_cols, SEQ_LEN, pad=True)\n",
    "\n",
    "print(f\"‚úÖ Train Sequences: {X_train_seq.shape} | Labels: {y_train_seq.shape}\")\n",
    "print(f\"‚úÖ Val Sequences:   {X_val_seq.shape}   | Labels: {y_val_seq.shape}\")\n",
    "print(f\"‚úÖ Test Sequences:  {X_test_seq.shape}   | Labels: {y_test_seq.shape}\")\n",
    "\n",
    "SEQ_DIR = PROCESSED_DIR / 'sequences'\n",
    "SEQ_DIR.mkdir(exist_ok=True)\n",
    "np.save(SEQ_DIR / 'X_train_seq.npy', X_train_seq)\n",
    "np.save(SEQ_DIR / 'y_train_seq.npy', y_train_seq)\n",
    "np.save(SEQ_DIR / 'X_val_seq.npy',   X_val_seq)\n",
    "np.save(SEQ_DIR / 'y_val_seq.npy',   y_val_seq)\n",
    "np.save(SEQ_DIR / 'X_test_seq.npy',  X_test_seq)\n",
    "np.save(SEQ_DIR / 'y_test_seq.npy',  y_test_seq)\n",
    "print(f\"‚úÖ Saved sequences to {SEQ_DIR} (ALL subsets combined)\")\n",
    "\n",
    "print(f\"\\nüéâ NB01 COMPLETE ‚Äî processed ALL 4 C-MAPSS subsets (FD001‚ÄìFD004).\")\n",
    "print(f\"   Total engines: ~708 | Total rows: ~{len(full_train_df) + len(full_test_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5c949",
   "metadata": {},
   "source": [
    "# Section 6: Download and Select LogHub Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f83b5e",
   "metadata": {},
   "source": [
    "## LogHub Datasets\n",
    "\n",
    "LogHub is a collection of public log datasets from real systems:\n",
    "\n",
    "| Dataset | System | Size | Entries | Characteristics |\n",
    "|---------|--------|------|---------|----------|\n",
    "| HDFS | Hadoop | ~55 MB | 11M | Distributed file system logs |\n",
    "| BGL | Blue Gene/L | ~700 MB | 4.7M | Supercomputer logs |\n",
    "| OpenStack | Cloud | ~300 MB | 207k | Cloud infrastructure logs |\n",
    "| Android | Mobile | Large | 1.5M | Mobile OS logs |\n",
    "\n",
    "We'll focus on **HDFS** and **BGL** for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c67437",
   "metadata": {},
   "source": [
    "# Section 7: Normalize and Parse Log Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4bd7b",
   "metadata": {},
   "source": [
    "## Example: Parsing synthetic log data\n",
    "\n",
    "Since we don't have the actual LogHub data yet, we'll demonstrate the parsing pipeline with example logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e46d13",
   "metadata": {},
   "source": [
    "# Section 8: Convert Logs to Incident Narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9dd0f",
   "metadata": {},
   "source": [
    "# Section 9: Store Clean Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed858b",
   "metadata": {},
   "source": [
    "\n",
    "# Summary: PHASE 2 Complete ‚úÖ\n",
    "\n",
    "## What We Accomplished:\n",
    "\n",
    "### Time-Series Data (NASA C-MAPSS ‚Äî ALL 4 Subsets)\n",
    "- ‚úÖ Loaded and parsed **ALL 4 C-MAPSS subsets** (FD001‚ÄìFD004):\n",
    "  - **FD001**: 100 engines, 1 op condition, 1 fault mode\n",
    "  - **FD002**: 260 engines, 6 op conditions, 1 fault mode\n",
    "  - **FD003**: 100 engines, 1 op condition, 2 fault modes\n",
    "  - **FD004**: 248 engines, 6 op conditions, 2 fault modes\n",
    "- ‚úÖ Combined into single DataFrames with `subset` + `dataset_id` columns\n",
    "- ‚úÖ Applied scaling strategy per subset:\n",
    "  - FD001/FD003: Global MinMaxScaler (single operating condition)\n",
    "  - FD002/FD004: KMeans regime-based scaling (6 operating conditions)\n",
    "- ‚úÖ Created RUL labels (piecewise linear, clipped at 125)\n",
    "- ‚úÖ Split train/val by engine using GroupShuffleSplit (80/20, no leakage)\n",
    "- ‚úÖ Generated sliding-window sequences (length=30) for deep learning\n",
    "\n",
    "### Text Data (LogHub)\n",
    "- ‚úÖ Demonstrated log parsing pipeline\n",
    "- ‚úÖ Normalized log messages, grouped into incident bursts\n",
    "- ‚úÖ Generated incident narratives & synthetic maintenance reports\n",
    "- ‚úÖ Stored clean text corpus\n",
    "\n",
    "## Outputs:\n",
    "\n",
    "**Time-Series Data (ALL subsets combined):**\n",
    "- `data/processed/train_FD001.csv` ‚Äî Training data (80% of engines from ALL 4 subsets)\n",
    "- `data/processed/val_FD001.csv` ‚Äî Validation data (20% of engines from ALL 4 subsets)\n",
    "- `data/processed/test_FD001.csv` ‚Äî Test data (ALL 4 subsets, separate engine set)\n",
    "- `data/processed/sequences/` ‚Äî Numpy sequences for LSTM/TCN (ALL subsets)\n",
    "- `models/feature_scaler.joblib` ‚Äî Per-subset scalers\n",
    "- `reports/figures/eda/` ‚Äî Sensor visualizations, correlation heatmaps\n",
    "\n",
    "**Text Data:**\n",
    "- `data/interim/combined_text_corpus.csv` ‚Äî Parsed logs\n",
    "- `data/processed/text_corpus/` ‚Äî Cleaned logs, incidents, narratives\n",
    "\n",
    "## Next Steps (PHASE 3+):\n",
    "- Feature engineering pipeline (rolling aggregates, degradation signals)\n",
    "- Baseline 1: XGBoost + Random Forest on ALL subsets\n",
    "- Baseline 2: LSTM + TCN deep learning models\n",
    "- Anomaly detection, RAG pipeline, Agentic AI orchestration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
